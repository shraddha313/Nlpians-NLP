{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning and preparaing dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# List of computer science-related keywords\n",
    "cs_keywords = [\n",
    "    \"algorithm\", \"data structure\", \"programming\", \"computer\", \"software\", \n",
    "    \"network\", \"database\", \"machine learning\", \"AI\", \"artificial intelligence\",\n",
    "    \"computing\", \"code\", \"coding\", \"neural network\", \"deep learning\", \"blockchain\",\n",
    "    \"runtime\", \"compiler\", \"operating system\", \"API\", \"application\", \"hardware\",\n",
    "    \"CPU\", \"GPU\", \"RAM\", \"disk\", \"cache\", \"cloud computing\", \"cybersecurity\", \"encryption\"\n",
    "    # ... add more keywords as needed\n",
    "]\n",
    "\n",
    "def is_cs_related(text):\n",
    "    # Convert text to lowercase and check if any of the keywords are present\n",
    "    return any(keyword in text.lower() for keyword in cs_keywords)\n",
    "\n",
    "\n",
    "selected_data = []\n",
    "\n",
    "# Open the JSON file for reading\n",
    "with open(\"s2ag.valid.0\", \"r\") as json_file:\n",
    "    for line in json_file:\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            # Check if the data has 'text' and 'id' keys and if the text is CS-related\n",
    "            if 'text' in data and 'id' in data and is_cs_related(data['text']):\n",
    "                selected_data.append(data)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Invalid JSON data:\", line)\n",
    "\n",
    "# Shuffle the selected data to ensure randomness\n",
    "random.shuffle(selected_data)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_data = selected_data[:3000]\n",
    "test_data = selected_data[3000:4000]\n",
    "\n",
    "# Now you have 'train_data' and 'test_data' containing computer science-related entries\n",
    "# Each entry is a dictionary with 'id' and 'text' keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume train_data is a list of dictionaries with keys 'id' and 'text'\n",
    "\n",
    "# Loop through and create a formatted string for each entry\n",
    "output_string = \"\"\n",
    "for entry in train_data:\n",
    "    entry_string = \"ID: \" + str(entry[\"id\"]) + \"\\n\" + \"Text: \" + entry[\"text\"] + \"\\n\\n\"\n",
    "    output_string += entry_string\n",
    "\n",
    "# Save to a txt file with utf-8 encoding\n",
    "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in train_data:\n",
    "    print(\"ID:\", entry[\"id\"])\n",
    "    print(\"Text:\", entry[\"text\"])\n",
    "    print()  # Add an empty line for separation between entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the txt file\n",
    "with open(\"output.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Modify the lines as needed\n",
    "for i in range(len(lines)):\n",
    "    line = lines[i].strip()  # Remove leading/trailing white spaces\n",
    "    if line.startswith(\"Text:\"):\n",
    "        # Check if the line doesn't end with punctuation (you can extend this list if needed)\n",
    "        if not (line.endswith('.') or line.endswith('!') or line.endswith('?')):\n",
    "            lines[i] = line + '.\\n'\n",
    "\n",
    "# Save the modified content back to the txt file\n",
    "with open(\"output_modified.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.writelines(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the txt file\n",
    "with open(\"output_modified.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Initialize a list to store the formatted content\n",
    "formatted_lines = []\n",
    "\n",
    "# Initialize buffer for collecting text\n",
    "text_buffer = []\n",
    "\n",
    "# Loop through the lines and format them\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if line.startswith(\"ID:\"):\n",
    "        # If there's content in the buffer, append it as a single paragraph\n",
    "        if text_buffer:\n",
    "            formatted_lines.append(\" \".join(text_buffer))\n",
    "            text_buffer = []\n",
    "        formatted_lines.append(line)\n",
    "    else:\n",
    "        # Collect lines in buffer to eventually combine them into a paragraph\n",
    "        text_buffer.append(line.replace(\"Text:\", \"\").strip())\n",
    "\n",
    "# Add any remaining text in the buffer as a paragraph\n",
    "if text_buffer:\n",
    "    formatted_lines.append(\" \".join(text_buffer))\n",
    "\n",
    "# Save the formatted content back to a new txt file\n",
    "with open(\"output_paragraphed.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(formatted_lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the txt file\n",
    "with open(\"output_paragraphed.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Filter out lines that start with \"ID:\"\n",
    "filtered_lines = [line for line in lines if not line.startswith(\"ID:\")]\n",
    "\n",
    "# Save the filtered content back to the txt file (or a new one if you prefer)\n",
    "with open(\"output_paragraphed2.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.writelines(filtered_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Read the text from the file\n",
    "with open(\"output_paragraphed2.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    whole_text = file.read()\n",
    "\n",
    "nlp = English()  # Create an English language object\n",
    "sentencizer = nlp.add_pipe(\"sentencizer\")  # Add the sentencizer component to the pipeline\n",
    "\n",
    "# Break text into chunks of 500,000 characters each\n",
    "chunk_size = 500000\n",
    "text_chunks = [whole_text[i:i + chunk_size] for i in range(0, len(whole_text), chunk_size)]\n",
    "\n",
    "all_sentences = []\n",
    "for chunk in text_chunks:\n",
    "    doc = nlp(chunk)  # Process the chunk\n",
    "    # Extract sentences from the doc and extend the all_sentences list\n",
    "    all_sentences.extend([sent.text for sent in doc.sents])\n",
    "\n",
    "# Save the sentences to a new txt file\n",
    "with open(\"sentences_output.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for sentence in all_sentences:\n",
    "        file.write(sentence + \"\\n\\n\")  # Each sentence on a new line with a blank line in between\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Filtering and Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    # Remove bullets, apostrophes, hyphens, and enumerators without merging words\n",
    "    text = re.sub(r'[\\•\\’\\'\\-\\–]', ' ', text)  # Including the apostrophe ' now\n",
    "    text = re.sub(r'\\d+\\.', '', text)  # Removing enumerators\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from word2number import w2n\n",
    "\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    return url_pattern.sub('', text)\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    # Remove bullets, apostrophes, hyphens, and enumerators without merging words\n",
    "    text = re.sub(r'[\\•\\’\\-\\–]', ' ', text) # Replacing with space\n",
    "    text = re.sub(r'\\d+\\.', '', text)  # Removing enumerators\n",
    "    return text\n",
    "\n",
    "def process_numericals(text):\n",
    "    # If the line is just a number or a decimal, return an empty string\n",
    "    if re.fullmatch(r'\\d+(\\.\\d+)?', text.strip()):\n",
    "        return ''\n",
    "    # Convert other numbers in the sentence to text\n",
    "    text = re.sub(r'\\b\\d+\\b', lambda m: str(w2n.word_to_num(m.group())), text)\n",
    "    return text\n",
    "\n",
    "\n",
    "with open(\"sentences_output.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "processed_lines = []\n",
    "for line in lines:\n",
    "    line = remove_urls(line)\n",
    "    line = remove_special_characters(line)\n",
    "    line = process_numericals(line)\n",
    "    processed_lines.append(line)\n",
    "\n",
    "with open(\"processed_file.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.writelines(processed_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words as nltk_words\n",
    "\n",
    "nltk.download('words')\n",
    "\n",
    "def is_valid_word(word, word_list):\n",
    "    # Convert to lowercase and check if the word exists in the word list\n",
    "    return word.lower() in word_list\n",
    "\n",
    "def process_file(filename):\n",
    "    # Load English words\n",
    "    english_words = set(nltk_words.words())\n",
    "\n",
    "    # Open the file with utf-8 encoding\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    processed_sentences = []\n",
    "    all_words = set()\n",
    "\n",
    "    for line in lines:\n",
    "        # Convert to lowercase\n",
    "        line = line.strip().lower()\n",
    "\n",
    "        # Split into words and remove the trailing period\n",
    "        words = [w.rstrip('.') for w in line.split()]\n",
    "\n",
    "        # Filter out invalid words\n",
    "        valid_words = [word for word in words if is_valid_word(word, english_words)]\n",
    "\n",
    "        if valid_words:  # Only add sentences that have at least one valid word\n",
    "            processed_sentence = ' '.join(valid_words)\n",
    "            processed_sentences.append(processed_sentence)\n",
    "            all_words.update(valid_words)\n",
    "\n",
    "    # Save to new file\n",
    "    with open('processed_output2.txt', 'w', encoding='utf-8') as f:\n",
    "        for sentence in processed_sentences:\n",
    "            f.write(sentence + '\\n')\n",
    "\n",
    "    # Get ordered unique sentences and words\n",
    "    unique_sentences = sorted(set(processed_sentences))\n",
    "    unique_words = sorted(all_words)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Ordered Unique Sentences:\")\n",
    "    for sentence in unique_sentences:\n",
    "        print(sentence)\n",
    "    print(\"\\nSet of Words:\")\n",
    "    for word in unique_words:\n",
    "        print(word)\n",
    "    print(\"\\nLength of Unique Sentences:\", len(unique_sentences))\n",
    "    print(\"Length of Set of Words:\", len(unique_words))\n",
    "\n",
    "process_file('processed_file.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Designing input matrix ,tf-idf matrix,output matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "def process_file(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Removing duplicates while maintaining order\n",
    "    unique_sentences = sorted(set(lines), key=lines.index)\n",
    "    \n",
    "    # Extract all unique words, ensuring lowercase representation\n",
    "    all_words = set()\n",
    "    for sentence in unique_sentences:\n",
    "        for word in sentence.split():\n",
    "            all_words.add(word.lower())\n",
    "\n",
    "    return unique_sentences, sorted(all_words)\n",
    "\n",
    "unique_sentences, unique_words = process_file('new.txt')\n",
    "word_to_index = {word: idx for idx, word in enumerate(unique_words)}\n",
    "\n",
    "num_sentences = len(unique_sentences)\n",
    "num_words = len(unique_words)\n",
    "\n",
    "input_matrix = lil_matrix((num_sentences, num_words), dtype=int)\n",
    "output_matrix = lil_matrix((num_sentences, num_words), dtype=int)\n",
    "\n",
    "context_window = 3\n",
    "\n",
    "for idx, sentence in enumerate(unique_sentences):\n",
    "    # Input matrix\n",
    "    for word in sentence.split():\n",
    "        input_matrix[idx, word_to_index[word.lower()]] = 1\n",
    "\n",
    "    # Output matrix\n",
    "    context_start = max(0, idx - (context_window // 2))\n",
    "    context_end = min(num_sentences, idx + (context_window // 2) + 1)\n",
    "    \n",
    "    for i in range(context_start, context_end):\n",
    "        if i != idx:\n",
    "            for word in unique_sentences[i].split():\n",
    "                output_matrix[idx, word_to_index[word.lower()]] = 1\n",
    "\n",
    "# Converting to CSR format for efficient row-wise operations\n",
    "input_matrix = input_matrix.tocsr()\n",
    "output_matrix = output_matrix.tocsr()\n",
    "\n",
    "# print(\"Input Matrix - First Row:\")\n",
    "# print(input_matrix[0].toarray())\n",
    "# print(\"\\nOutput Matrix - First Row:\")\n",
    "# print(output_matrix[0].toarray())\n",
    "\n",
    "# # Non-zero entries for the first row\n",
    "# non_zero_input = input_matrix[0].nonzero()[1]\n",
    "# non_zero_output = output_matrix[0].nonzero()[1]\n",
    "\n",
    "# print(\"\\nNon-Zero Entries for First Row of Input Matrix:\", non_zero_input)\n",
    "# print(\"Words corresponding to non-zero entries in the Input Matrix:\", [unique_words[idx] for idx in non_zero_input])\n",
    "\n",
    "# print(\"\\nNon-Zero Entries for First Row of Output Matrix:\", non_zero_output)\n",
    "# print(\"Words corresponding to non-zero entries in the Output Matrix:\", [unique_words[idx] for idx in non_zero_output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(unique_sentences))\n",
    "# print(len(unique_words))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "# Step 1: Calculate IDF for each word in the vocabulary\n",
    "\n",
    "word_sentence_indices = defaultdict(set)\n",
    "\n",
    "for idx, sentence in enumerate(unique_sentences):\n",
    "    for word in set(sentence.split()):  # Using set to avoid counting a word multiple times in the same sentence\n",
    "        word_sentence_indices[word.lower()].add(idx)\n",
    "\n",
    "idf = {}\n",
    "for word, sentence_indices in word_sentence_indices.items():\n",
    "    idf[word] = math.log(len(unique_sentences) / len(sentence_indices))\n",
    "\n",
    "# Step 2: Calculate TF for each word in each sentence based on the context window and compute TF-IDF\n",
    "\n",
    "tf_idf_matrix = lil_matrix((len(unique_sentences), len(unique_words)), dtype=float)\n",
    "\n",
    "context_window_size = 3  # -1, 0, 1\n",
    "\n",
    "for idx, sentence in enumerate(unique_sentences):\n",
    "    word_counts = defaultdict(int)\n",
    "    \n",
    "    # Calculate TF (Term Frequency) with the context window\n",
    "    context_start = max(0, idx - (context_window_size // 2))\n",
    "    context_end = min(len(unique_sentences), idx + (context_window_size // 2) + 1)\n",
    "    \n",
    "    total_words_in_context = 0\n",
    "    for i in range(context_start, context_end):\n",
    "        total_words_in_context += len(unique_sentences[i].split())\n",
    "        for word in unique_sentences[i].split():\n",
    "            word_counts[word.lower()] += 1\n",
    "    \n",
    "\n",
    "\n",
    "    for word, count in word_counts.items():\n",
    "        tf = count / total_words_in_context\n",
    "        tf_idf_matrix[idx, unique_words.index(word)] = tf * idf[word]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METHOD:1Taking input matrix and outputmatrix where input matrix contain 1 where targer sentence words are present "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def softmax(X):\n",
    "    X_exp = tf.exp(X)\n",
    "    partition = tf.reduce_sum(X_exp, 1, keepdims=True)\n",
    "    return X_exp / partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "def process_file(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Removing duplicates while maintaining order\n",
    "    unique_sentences = sorted(set(lines), key=lines.index)\n",
    "    \n",
    "    # Extract all unique words, ensuring lowercase representation\n",
    "    all_words = set()\n",
    "    for sentence in unique_sentences:\n",
    "        for word in sentence.split():\n",
    "            all_words.add(word.lower())\n",
    "\n",
    "    return unique_sentences, sorted(all_words)\n",
    "\n",
    "unique_sentences, unique_words = process_file('new.txt')\n",
    "word_to_index = {word: idx for idx, word in enumerate(unique_words)}\n",
    "\n",
    "num_sentences = len(unique_sentences)\n",
    "num_words = len(unique_words)\n",
    "\n",
    "input_matrix = lil_matrix((num_sentences, num_words), dtype=int)\n",
    "output_matrix = lil_matrix((num_sentences, num_words), dtype=int)\n",
    "\n",
    "context_window = 3\n",
    "\n",
    "for idx, sentence in enumerate(unique_sentences):\n",
    "    # Input matrix\n",
    "    for word in sentence.split():\n",
    "        input_matrix[idx, word_to_index[word.lower()]] = 1\n",
    "\n",
    "    # Output matrix\n",
    "    context_start = max(0, idx - (context_window // 2))\n",
    "    context_end = min(num_sentences, idx + (context_window // 2) + 1)\n",
    "    \n",
    "    for i in range(context_start, context_end):\n",
    "        if i != idx:\n",
    "            for word in unique_sentences[i].split():\n",
    "                output_matrix[idx, word_to_index[word.lower()]] = 1\n",
    "\n",
    "# Converting to CSR format for efficient row-wise operations\n",
    "input_matrix = input_matrix.tocsr()\n",
    "output_matrix = output_matrix.tocsr()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "vocab_size = input_matrix.shape[1]\n",
    "num_hiddens = 25\n",
    "\n",
    "W1 = tf.Variable(tf.random.normal(shape=(vocab_size, num_hiddens), mean=0, stddev=0.01))\n",
    "W2 = tf.Variable(tf.random.normal(shape=(num_hiddens, vocab_size), mean=0, stddev=0.01))\n",
    "\n",
    "params = [W1, W2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.398483, shape=(), dtype=float32)\n",
      "tf.Tensor(9.398483, shape=(), dtype=float32)\n",
      "tf.Tensor(9.398483, shape=(), dtype=float32)\n",
      "tf.Tensor(9.398483, shape=(), dtype=float32)\n",
      "tf.Tensor(9.398483, shape=(), dtype=float32)\n",
      "tf.Tensor(9.398483, shape=(), dtype=float32)\n",
      "tf.Tensor(9.398483, shape=(), dtype=float32)\n",
      "tf.Tensor(9.398483, shape=(), dtype=float32)\n",
      "tf.Tensor(9.398483, shape=(), dtype=float32)\n",
      "tf.Tensor(9.398483, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from scipy.sparse import lil_matrix\n",
    "import random\n",
    "\n",
    "# ...[Your previous function and data preparation definitions]\n",
    "\n",
    "def net(X):\n",
    "    X = tf.reshape(tf.cast(X, dtype=tf.float32), (-1, vocab_size))\n",
    "    a1 = tf.matmul(X, W1)  # Replaced input_matrix with X\n",
    "    a2 = tf.matmul(a1, W2)\n",
    "    return softmax(a2)\n",
    "\n",
    "def negative_log_likelihood(y_hat, y):\n",
    "    # Compute the negative log likelihood for true class predictions\n",
    "    return -tf.reduce_mean(tf.math.log(tf.boolean_mask(y_hat, y) + 1e-10))\n",
    "\n",
    "def sgd(params, grads, lr):\n",
    "    \"\"\"Gradient descent.\"\"\"\n",
    "    for param, grad in zip(params, grads):\n",
    "        param.assign_sub(lr * grad)\n",
    "\n",
    "def softmax(X):\n",
    "    X_exp = tf.exp(X)\n",
    "    partition = tf.reduce_sum(X_exp, 1, keepdims=True)\n",
    "    return X_exp / partition \n",
    "\n",
    "# ...[Your previous data loading and processing definitions]\n",
    "\n",
    "# Converting sparse matrices to dense tensors for TensorFlow operations\n",
    "input_tensor = tf.convert_to_tensor(input_matrix.toarray(), dtype=tf.float32)\n",
    "output_tensor = tf.convert_to_tensor(output_matrix.toarray(), dtype=tf.float32)\n",
    "\n",
    "lr = 0.1\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_hat = net(input_tensor)\n",
    "        l = negative_log_likelihood(y_hat, output_tensor)\n",
    "        print(l)\n",
    "    grads = tape.gradient(l, params)\n",
    "    sgd(params, grads, lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(tf.Module):\n",
    "    def __init__(self, W1, W2):\n",
    "        self.W1 = tf.Variable(W1)\n",
    "        self.W2 = tf.Variable(W2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_embeddings\\assets\n"
     ]
    }
   ],
   "source": [
    "embedding_module = Embeddings(W1, W2)\n",
    "saved_model_path = \"./saved_embeddings\"\n",
    "tf.saved_model.save(embedding_module, saved_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_module = tf.saved_model.load(saved_model_path)\n",
    "W1_loaded = loaded_module.W1\n",
    "W2_loaded = loaded_module.W2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3510478\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(vector_a, vector_b):\n",
    "    dot_product = tf.reduce_sum(tf.multiply(vector_a, vector_b))\n",
    "    norm_a = tf.norm(vector_a)\n",
    "    norm_b = tf.norm(vector_b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "# Example: Compute cosine similarity between two rows (i.e., two word embeddings) from W1_loaded\n",
    "similarity_score = cosine_similarity(W1_loaded[1], W1_loaded[5])\n",
    "print(similarity_score.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between 'abdominal' and 'ability': 0.1905229389667511\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(vector_a, vector_b):\n",
    "    dot_product = tf.reduce_sum(tf.multiply(vector_a, vector_b))\n",
    "    norm_a = tf.norm(vector_a)\n",
    "    norm_b = tf.norm(vector_b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "# No need for the line \"unique_words = list(idx.keys())\", remove it.\n",
    "\n",
    "# Example: Compute cosine similarity between two rows (i.e., two word embeddings) from W1_loaded\n",
    "index_a = 5\n",
    "index_b = 10\n",
    "similarity_score = cosine_similarity(W1_loaded[index_a], W1_loaded[index_b])\n",
    "\n",
    "word_a = unique_words[index_a]\n",
    "word_b = unique_words[index_b]\n",
    "\n",
    "print(f\"Cosine Similarity between '{word_a}' and '{word_b}': {similarity_score.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between 'absolutely' and 'absorb': 0.038716256618499756\n",
      "Cosine Similarity between 'absolutely' and 'absorbable': 0.34109070897102356\n",
      "Cosine Similarity between 'absolutely' and 'absorbed': -0.36461934447288513\n",
      "Cosine Similarity between 'absolutely' and 'absorber': 0.10269894450902939\n",
      "Cosine Similarity between 'absolutely' and 'absorbing': 0.14536257088184357\n",
      "Cosine Similarity between 'absolutely' and 'absorption': -0.12070897221565247\n",
      "Cosine Similarity between 'absolutely' and 'absorptive': 0.0826815515756607\n",
      "Cosine Similarity between 'absolutely' and 'abstain': 0.27033525705337524\n",
      "Cosine Similarity between 'absolutely' and 'abstract': -0.20421965420246124\n",
      "Cosine Similarity between 'absolutely' and 'abstracted': 0.05708847939968109\n",
      "Cosine Similarity between 'absolutely' and 'abstraction': 0.010739210061728954\n",
      "Cosine Similarity between 'absolutely' and 'abstractly': -0.12247917056083679\n",
      "Cosine Similarity between 'absolutely' and 'abu': -0.010429462417960167\n",
      "Cosine Similarity between 'absolutely' and 'abundance': -0.36192774772644043\n",
      "Cosine Similarity between 'absolutely' and 'abundant': -0.02400108054280281\n",
      "Cosine Similarity between 'absolutely' and 'abundantly': -0.3205474615097046\n",
      "Cosine Similarity between 'absolutely' and 'abuse': -0.19700202345848083\n",
      "Cosine Similarity between 'absolutely' and 'abusive': 0.22161678969860077\n",
      "Cosine Similarity between 'absolutely' and 'academic': -0.3436732590198517\n",
      "Cosine Similarity between 'absolutely' and 'academy': -0.11736245453357697\n",
      "Cosine Similarity between 'absorb' and 'absorbable': 0.4491211175918579\n",
      "Cosine Similarity between 'absorb' and 'absorbed': 0.09066633880138397\n",
      "Cosine Similarity between 'absorb' and 'absorber': 0.3093266189098358\n",
      "Cosine Similarity between 'absorb' and 'absorbing': 0.08446488529443741\n",
      "Cosine Similarity between 'absorb' and 'absorption': 0.13402631878852844\n",
      "Cosine Similarity between 'absorb' and 'absorptive': -0.32933175563812256\n",
      "Cosine Similarity between 'absorb' and 'abstain': 0.2448464334011078\n",
      "Cosine Similarity between 'absorb' and 'abstract': -0.07848793268203735\n",
      "Cosine Similarity between 'absorb' and 'abstracted': -0.005189638119190931\n",
      "Cosine Similarity between 'absorb' and 'abstraction': 0.08829731494188309\n",
      "Cosine Similarity between 'absorb' and 'abstractly': 0.3274310231208801\n",
      "Cosine Similarity between 'absorb' and 'abu': 0.12318280339241028\n",
      "Cosine Similarity between 'absorb' and 'abundance': 0.3140083849430084\n",
      "Cosine Similarity between 'absorb' and 'abundant': -0.29045066237449646\n",
      "Cosine Similarity between 'absorb' and 'abundantly': -0.012143775820732117\n",
      "Cosine Similarity between 'absorb' and 'abuse': -0.003012116299942136\n",
      "Cosine Similarity between 'absorb' and 'abusive': -0.028675716370344162\n",
      "Cosine Similarity between 'absorb' and 'academic': 0.05401807650923729\n",
      "Cosine Similarity between 'absorb' and 'academy': 0.37870702147483826\n",
      "Cosine Similarity between 'absorbable' and 'absorbed': -0.1662854105234146\n",
      "Cosine Similarity between 'absorbable' and 'absorber': 0.16662737727165222\n",
      "Cosine Similarity between 'absorbable' and 'absorbing': 0.15529794991016388\n",
      "Cosine Similarity between 'absorbable' and 'absorption': 0.04924939572811127\n",
      "Cosine Similarity between 'absorbable' and 'absorptive': -0.2057822048664093\n",
      "Cosine Similarity between 'absorbable' and 'abstain': -0.04318036511540413\n",
      "Cosine Similarity between 'absorbable' and 'abstract': 0.17459708452224731\n",
      "Cosine Similarity between 'absorbable' and 'abstracted': 0.43673422932624817\n",
      "Cosine Similarity between 'absorbable' and 'abstraction': 0.06605414301156998\n",
      "Cosine Similarity between 'absorbable' and 'abstractly': -0.09820208698511124\n",
      "Cosine Similarity between 'absorbable' and 'abu': 0.32785752415657043\n",
      "Cosine Similarity between 'absorbable' and 'abundance': -0.2215522974729538\n",
      "Cosine Similarity between 'absorbable' and 'abundant': 0.043617986142635345\n",
      "Cosine Similarity between 'absorbable' and 'abundantly': -0.02566644549369812\n",
      "Cosine Similarity between 'absorbable' and 'abuse': -0.35287579894065857\n",
      "Cosine Similarity between 'absorbable' and 'abusive': 0.0032411497086286545\n",
      "Cosine Similarity between 'absorbable' and 'academic': -0.2974412739276886\n",
      "Cosine Similarity between 'absorbable' and 'academy': 0.6214292645454407\n",
      "Cosine Similarity between 'absorbed' and 'absorber': 0.3453669250011444\n",
      "Cosine Similarity between 'absorbed' and 'absorbing': 0.11103525012731552\n",
      "Cosine Similarity between 'absorbed' and 'absorption': 0.32914766669273376\n",
      "Cosine Similarity between 'absorbed' and 'absorptive': -0.22383415699005127\n",
      "Cosine Similarity between 'absorbed' and 'abstain': -0.30987757444381714\n",
      "Cosine Similarity between 'absorbed' and 'abstract': 0.16035467386245728\n",
      "Cosine Similarity between 'absorbed' and 'abstracted': 0.10026908665895462\n",
      "Cosine Similarity between 'absorbed' and 'abstraction': -0.009569778107106686\n",
      "Cosine Similarity between 'absorbed' and 'abstractly': 0.37293070554733276\n",
      "Cosine Similarity between 'absorbed' and 'abu': 0.2242388129234314\n",
      "Cosine Similarity between 'absorbed' and 'abundance': 0.1914733648300171\n",
      "Cosine Similarity between 'absorbed' and 'abundant': 0.01902916096150875\n",
      "Cosine Similarity between 'absorbed' and 'abundantly': 0.2968914806842804\n",
      "Cosine Similarity between 'absorbed' and 'abuse': 0.08478391915559769\n",
      "Cosine Similarity between 'absorbed' and 'abusive': -0.18758408725261688\n",
      "Cosine Similarity between 'absorbed' and 'academic': 0.23419944941997528\n",
      "Cosine Similarity between 'absorbed' and 'academy': 0.1411716639995575\n",
      "Cosine Similarity between 'absorber' and 'absorbing': -0.19013682007789612\n",
      "Cosine Similarity between 'absorber' and 'absorption': 0.020033640787005424\n",
      "Cosine Similarity between 'absorber' and 'absorptive': -0.1383257508277893\n",
      "Cosine Similarity between 'absorber' and 'abstain': -0.28526169061660767\n",
      "Cosine Similarity between 'absorber' and 'abstract': 0.2956824004650116\n",
      "Cosine Similarity between 'absorber' and 'abstracted': 0.14872877299785614\n",
      "Cosine Similarity between 'absorber' and 'abstraction': 0.12489757686853409\n",
      "Cosine Similarity between 'absorber' and 'abstractly': 0.27117839455604553\n",
      "Cosine Similarity between 'absorber' and 'abu': -0.014907445758581161\n",
      "Cosine Similarity between 'absorber' and 'abundance': -0.006303655914962292\n",
      "Cosine Similarity between 'absorber' and 'abundant': 0.19750334322452545\n",
      "Cosine Similarity between 'absorber' and 'abundantly': -0.1099235787987709\n",
      "Cosine Similarity between 'absorber' and 'abuse': 0.27936968207359314\n",
      "Cosine Similarity between 'absorber' and 'abusive': 0.21237510442733765\n",
      "Cosine Similarity between 'absorber' and 'academic': 0.3469659984111786\n",
      "Cosine Similarity between 'absorber' and 'academy': 0.17662371695041656\n",
      "Cosine Similarity between 'absorbing' and 'absorption': 0.19965142011642456\n",
      "Cosine Similarity between 'absorbing' and 'absorptive': 0.2346678525209427\n",
      "Cosine Similarity between 'absorbing' and 'abstain': 0.3578653931617737\n",
      "Cosine Similarity between 'absorbing' and 'abstract': -0.05544676631689072\n",
      "Cosine Similarity between 'absorbing' and 'abstracted': -0.0677228718996048\n",
      "Cosine Similarity between 'absorbing' and 'abstraction': 0.19227588176727295\n",
      "Cosine Similarity between 'absorbing' and 'abstractly': 0.3607746362686157\n",
      "Cosine Similarity between 'absorbing' and 'abu': 0.2871154248714447\n",
      "Cosine Similarity between 'absorbing' and 'abundance': -0.19953350722789764\n",
      "Cosine Similarity between 'absorbing' and 'abundant': -0.23476579785346985\n",
      "Cosine Similarity between 'absorbing' and 'abundantly': -0.24146446585655212\n",
      "Cosine Similarity between 'absorbing' and 'abuse': -0.3047918379306793\n",
      "Cosine Similarity between 'absorbing' and 'abusive': -0.17019647359848022\n",
      "Cosine Similarity between 'absorbing' and 'academic': -0.3507044017314911\n",
      "Cosine Similarity between 'absorbing' and 'academy': 0.024249542504549026\n",
      "Cosine Similarity between 'absorption' and 'absorptive': 0.0050864745862782\n",
      "Cosine Similarity between 'absorption' and 'abstain': -0.2530892789363861\n",
      "Cosine Similarity between 'absorption' and 'abstract': 0.26981931924819946\n",
      "Cosine Similarity between 'absorption' and 'abstracted': 0.043840792030096054\n",
      "Cosine Similarity between 'absorption' and 'abstraction': 0.35245800018310547\n",
      "Cosine Similarity between 'absorption' and 'abstractly': 0.31772345304489136\n",
      "Cosine Similarity between 'absorption' and 'abu': 0.25046011805534363\n",
      "Cosine Similarity between 'absorption' and 'abundance': 0.368160218000412\n",
      "Cosine Similarity between 'absorption' and 'abundant': 0.10731195658445358\n",
      "Cosine Similarity between 'absorption' and 'abundantly': 0.029743347316980362\n",
      "Cosine Similarity between 'absorption' and 'abuse': -0.04447057843208313\n",
      "Cosine Similarity between 'absorption' and 'abusive': -0.2719881236553192\n",
      "Cosine Similarity between 'absorption' and 'academic': -0.05193479731678963\n",
      "Cosine Similarity between 'absorption' and 'academy': -0.16760693490505219\n",
      "Cosine Similarity between 'absorptive' and 'abstain': 0.025660768151283264\n",
      "Cosine Similarity between 'absorptive' and 'abstract': -0.02508137933909893\n",
      "Cosine Similarity between 'absorptive' and 'abstracted': -0.15010809898376465\n",
      "Cosine Similarity between 'absorptive' and 'abstraction': -0.07431387901306152\n",
      "Cosine Similarity between 'absorptive' and 'abstractly': -0.38468611240386963\n",
      "Cosine Similarity between 'absorptive' and 'abu': -0.5247617363929749\n",
      "Cosine Similarity between 'absorptive' and 'abundance': -0.06070096790790558\n",
      "Cosine Similarity between 'absorptive' and 'abundant': -0.07981124520301819\n",
      "Cosine Similarity between 'absorptive' and 'abundantly': -0.15767890214920044\n",
      "Cosine Similarity between 'absorptive' and 'abuse': -0.27602332830429077\n",
      "Cosine Similarity between 'absorptive' and 'abusive': 0.29146891832351685\n",
      "Cosine Similarity between 'absorptive' and 'academic': 0.058762408792972565\n",
      "Cosine Similarity between 'absorptive' and 'academy': -0.3723433315753937\n",
      "Cosine Similarity between 'abstain' and 'abstract': -0.22213785350322723\n",
      "Cosine Similarity between 'abstain' and 'abstracted': -0.28101757168769836\n",
      "Cosine Similarity between 'abstain' and 'abstraction': -0.1690898835659027\n",
      "Cosine Similarity between 'abstain' and 'abstractly': 0.012367846444249153\n",
      "Cosine Similarity between 'abstain' and 'abu': 0.07289472967386246\n",
      "Cosine Similarity between 'abstain' and 'abundance': 0.0577365979552269\n",
      "Cosine Similarity between 'abstain' and 'abundant': -0.21628092229366302\n",
      "Cosine Similarity between 'abstain' and 'abundantly': -0.38454240560531616\n",
      "Cosine Similarity between 'abstain' and 'abuse': -0.06982863694429398\n",
      "Cosine Similarity between 'abstain' and 'abusive': 0.2199673056602478\n",
      "Cosine Similarity between 'abstain' and 'academic': -0.3555516004562378\n",
      "Cosine Similarity between 'abstain' and 'academy': -0.16873976588249207\n",
      "Cosine Similarity between 'abstract' and 'abstracted': 0.2936181426048279\n",
      "Cosine Similarity between 'abstract' and 'abstraction': 0.18929491937160492\n",
      "Cosine Similarity between 'abstract' and 'abstractly': 0.09540865570306778\n",
      "Cosine Similarity between 'abstract' and 'abu': 0.04524315893650055\n",
      "Cosine Similarity between 'abstract' and 'abundance': -0.1284082531929016\n",
      "Cosine Similarity between 'abstract' and 'abundant': 0.5344817042350769\n",
      "Cosine Similarity between 'abstract' and 'abundantly': -0.20819991827011108\n",
      "Cosine Similarity between 'abstract' and 'abuse': -0.05533872917294502\n",
      "Cosine Similarity between 'abstract' and 'abusive': -0.19641748070716858\n",
      "Cosine Similarity between 'abstract' and 'academic': 0.03739118203520775\n",
      "Cosine Similarity between 'abstract' and 'academy': 0.12958739697933197\n",
      "Cosine Similarity between 'abstracted' and 'abstraction': 0.08424434065818787\n",
      "Cosine Similarity between 'abstracted' and 'abstractly': -0.11328018456697464\n",
      "Cosine Similarity between 'abstracted' and 'abu': 0.20815585553646088\n",
      "Cosine Similarity between 'abstracted' and 'abundance': -0.19165612757205963\n",
      "Cosine Similarity between 'abstracted' and 'abundant': 0.2077062875032425\n",
      "Cosine Similarity between 'abstracted' and 'abundantly': 0.19045405089855194\n",
      "Cosine Similarity between 'abstracted' and 'abuse': -0.2003413885831833\n",
      "Cosine Similarity between 'abstracted' and 'abusive': 0.017036939039826393\n",
      "Cosine Similarity between 'abstracted' and 'academic': 0.023114945739507675\n",
      "Cosine Similarity between 'abstracted' and 'academy': 0.14165151119232178\n",
      "Cosine Similarity between 'abstraction' and 'abstractly': 0.06197117269039154\n",
      "Cosine Similarity between 'abstraction' and 'abu': 0.07045795768499374\n",
      "Cosine Similarity between 'abstraction' and 'abundance': 0.22355180978775024\n",
      "Cosine Similarity between 'abstraction' and 'abundant': 0.053535331040620804\n",
      "Cosine Similarity between 'abstraction' and 'abundantly': -0.17619997262954712\n",
      "Cosine Similarity between 'abstraction' and 'abuse': -0.054901864379644394\n",
      "Cosine Similarity between 'abstraction' and 'abusive': -0.06789606064558029\n",
      "Cosine Similarity between 'abstraction' and 'academic': 0.1783832609653473\n",
      "Cosine Similarity between 'abstraction' and 'academy': -0.06750781089067459\n",
      "Cosine Similarity between 'abstractly' and 'abu': 0.2895813584327698\n",
      "Cosine Similarity between 'abstractly' and 'abundance': -0.019793085753917694\n",
      "Cosine Similarity between 'abstractly' and 'abundant': -0.10907654464244843\n",
      "Cosine Similarity between 'abstractly' and 'abundantly': -0.0038696718402206898\n",
      "Cosine Similarity between 'abstractly' and 'abuse': 0.4296407699584961\n",
      "Cosine Similarity between 'abstractly' and 'abusive': -0.5126270651817322\n",
      "Cosine Similarity between 'abstractly' and 'academic': 0.11245734989643097\n",
      "Cosine Similarity between 'abstractly' and 'academy': 0.13378378748893738\n",
      "Cosine Similarity between 'abu' and 'abundance': -0.035079553723335266\n",
      "Cosine Similarity between 'abu' and 'abundant': 0.03314078971743584\n",
      "Cosine Similarity between 'abu' and 'abundantly': -0.021009201183915138\n",
      "Cosine Similarity between 'abu' and 'abuse': 0.24901536107063293\n",
      "Cosine Similarity between 'abu' and 'abusive': -0.25470831990242004\n",
      "Cosine Similarity between 'abu' and 'academic': -0.3263486623764038\n",
      "Cosine Similarity between 'abu' and 'academy': 0.156795471906662\n",
      "Cosine Similarity between 'abundance' and 'abundant': -0.22946329414844513\n",
      "Cosine Similarity between 'abundance' and 'abundantly': 0.11434774100780487\n",
      "Cosine Similarity between 'abundance' and 'abuse': 0.1357264518737793\n",
      "Cosine Similarity between 'abundance' and 'abusive': -0.04408638924360275\n",
      "Cosine Similarity between 'abundance' and 'academic': 0.33159589767456055\n",
      "Cosine Similarity between 'abundance' and 'academy': -0.024038514122366905\n",
      "Cosine Similarity between 'abundant' and 'abundantly': 0.18666782975196838\n",
      "Cosine Similarity between 'abundant' and 'abuse': -0.062192484736442566\n",
      "Cosine Similarity between 'abundant' and 'abusive': 0.10171303153038025\n",
      "Cosine Similarity between 'abundant' and 'academic': 0.013464163057506084\n",
      "Cosine Similarity between 'abundant' and 'academy': -0.18423160910606384\n",
      "Cosine Similarity between 'abundantly' and 'abuse': -0.2115165889263153\n",
      "Cosine Similarity between 'abundantly' and 'abusive': -0.3076696991920471\n",
      "Cosine Similarity between 'abundantly' and 'academic': 0.20024511218070984\n",
      "Cosine Similarity between 'abundantly' and 'academy': 0.04158177226781845\n",
      "Cosine Similarity between 'abuse' and 'abusive': -0.012630527839064598\n",
      "Cosine Similarity between 'abuse' and 'academic': 0.36668670177459717\n",
      "Cosine Similarity between 'abuse' and 'academy': -0.08853431046009064\n",
      "Cosine Similarity between 'abusive' and 'academic': 0.10396833717823029\n",
      "Cosine Similarity between 'abusive' and 'academy': -0.14954841136932373\n",
      "Cosine Similarity between 'academic' and 'academy': 0.034867215901613235\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(vector_a, vector_b):\n",
    "    dot_product = tf.reduce_sum(tf.multiply(vector_a, vector_b))\n",
    "    norm_a = tf.norm(vector_a)\n",
    "    norm_b = tf.norm(vector_b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "# Loop through the indices from 30 to 50\n",
    "for index_a in range(30, 51):\n",
    "    for index_b in range(index_a+1, 51):  # This ensures we don't compare the word with itself\n",
    "        similarity_score = cosine_similarity(W1_loaded[index_a], W1_loaded[index_b])\n",
    "        \n",
    "        word_a = unique_words[index_a]\n",
    "        word_b = unique_words[index_b]\n",
    "        \n",
    "        print(f\"Cosine Similarity between '{word_a}' and '{word_b}': {similarity_score.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between 'absolutely' and 'absorb': 0.038716256618499756\n",
      "Cosine Similarity between 'absolutely' and 'absorbable': 0.34109070897102356\n",
      "Cosine Similarity between 'absolutely' and 'absorbed': -0.36461934447288513\n",
      "Cosine Similarity between 'absolutely' and 'absorber': 0.10269894450902939\n",
      "Cosine Similarity between 'absolutely' and 'absorbing': 0.14536257088184357\n",
      "Cosine Similarity between 'absolutely' and 'absorption': -0.12070897221565247\n",
      "Cosine Similarity between 'absolutely' and 'absorptive': 0.0826815515756607\n",
      "Cosine Similarity between 'absolutely' and 'abstain': 0.27033525705337524\n",
      "Cosine Similarity between 'absolutely' and 'abstract': -0.20421965420246124\n",
      "Cosine Similarity between 'absolutely' and 'abstracted': 0.05708847939968109\n",
      "Cosine Similarity between 'absolutely' and 'abstraction': 0.010739210061728954\n",
      "Cosine Similarity between 'absolutely' and 'abstractly': -0.12247917056083679\n",
      "Cosine Similarity between 'absolutely' and 'abu': -0.010429462417960167\n",
      "Cosine Similarity between 'absolutely' and 'abundance': -0.36192774772644043\n",
      "Cosine Similarity between 'absolutely' and 'abundant': -0.02400108054280281\n",
      "Cosine Similarity between 'absolutely' and 'abundantly': -0.3205474615097046\n",
      "Cosine Similarity between 'absolutely' and 'abuse': -0.19700202345848083\n",
      "Cosine Similarity between 'absolutely' and 'abusive': 0.22161678969860077\n",
      "Cosine Similarity between 'absolutely' and 'academic': -0.3436732590198517\n",
      "Cosine Similarity between 'absolutely' and 'academy': -0.11736245453357697\n",
      "Cosine Similarity between 'absorb' and 'absorbable': 0.4491211175918579\n",
      "Cosine Similarity between 'absorb' and 'absorbed': 0.09066633880138397\n",
      "Cosine Similarity between 'absorb' and 'absorber': 0.3093266189098358\n",
      "Cosine Similarity between 'absorb' and 'absorbing': 0.08446488529443741\n",
      "Cosine Similarity between 'absorb' and 'absorption': 0.13402631878852844\n",
      "Cosine Similarity between 'absorb' and 'absorptive': -0.32933175563812256\n",
      "Cosine Similarity between 'absorb' and 'abstain': 0.2448464334011078\n",
      "Cosine Similarity between 'absorb' and 'abstract': -0.07848793268203735\n",
      "Cosine Similarity between 'absorb' and 'abstracted': -0.005189638119190931\n",
      "Cosine Similarity between 'absorb' and 'abstraction': 0.08829731494188309\n",
      "Cosine Similarity between 'absorb' and 'abstractly': 0.3274310231208801\n",
      "Cosine Similarity between 'absorb' and 'abu': 0.12318280339241028\n",
      "Cosine Similarity between 'absorb' and 'abundance': 0.3140083849430084\n",
      "Cosine Similarity between 'absorb' and 'abundant': -0.29045066237449646\n",
      "Cosine Similarity between 'absorb' and 'abundantly': -0.012143775820732117\n",
      "Cosine Similarity between 'absorb' and 'abuse': -0.003012116299942136\n",
      "Cosine Similarity between 'absorb' and 'abusive': -0.028675716370344162\n",
      "Cosine Similarity between 'absorb' and 'academic': 0.05401807650923729\n",
      "Cosine Similarity between 'absorb' and 'academy': 0.37870702147483826\n",
      "Cosine Similarity between 'absorbable' and 'absorbed': -0.1662854105234146\n",
      "Cosine Similarity between 'absorbable' and 'absorber': 0.16662737727165222\n",
      "Cosine Similarity between 'absorbable' and 'absorbing': 0.15529794991016388\n",
      "Cosine Similarity between 'absorbable' and 'absorption': 0.04924939572811127\n",
      "Cosine Similarity between 'absorbable' and 'absorptive': -0.2057822048664093\n",
      "Cosine Similarity between 'absorbable' and 'abstain': -0.04318036511540413\n",
      "Cosine Similarity between 'absorbable' and 'abstract': 0.17459708452224731\n",
      "Cosine Similarity between 'absorbable' and 'abstracted': 0.43673422932624817\n",
      "Cosine Similarity between 'absorbable' and 'abstraction': 0.06605414301156998\n",
      "Cosine Similarity between 'absorbable' and 'abstractly': -0.09820208698511124\n",
      "Cosine Similarity between 'absorbable' and 'abu': 0.32785752415657043\n",
      "Cosine Similarity between 'absorbable' and 'abundance': -0.2215522974729538\n",
      "Cosine Similarity between 'absorbable' and 'abundant': 0.043617986142635345\n",
      "Cosine Similarity between 'absorbable' and 'abundantly': -0.02566644549369812\n",
      "Cosine Similarity between 'absorbable' and 'abuse': -0.35287579894065857\n",
      "Cosine Similarity between 'absorbable' and 'abusive': 0.0032411497086286545\n",
      "Cosine Similarity between 'absorbable' and 'academic': -0.2974412739276886\n",
      "Cosine Similarity between 'absorbable' and 'academy': 0.6214292645454407\n",
      "Cosine Similarity between 'absorbed' and 'absorber': 0.3453669250011444\n",
      "Cosine Similarity between 'absorbed' and 'absorbing': 0.11103525012731552\n",
      "Cosine Similarity between 'absorbed' and 'absorption': 0.32914766669273376\n",
      "Cosine Similarity between 'absorbed' and 'absorptive': -0.22383415699005127\n",
      "Cosine Similarity between 'absorbed' and 'abstain': -0.30987757444381714\n",
      "Cosine Similarity between 'absorbed' and 'abstract': 0.16035467386245728\n",
      "Cosine Similarity between 'absorbed' and 'abstracted': 0.10026908665895462\n",
      "Cosine Similarity between 'absorbed' and 'abstraction': -0.009569778107106686\n",
      "Cosine Similarity between 'absorbed' and 'abstractly': 0.37293070554733276\n",
      "Cosine Similarity between 'absorbed' and 'abu': 0.2242388129234314\n",
      "Cosine Similarity between 'absorbed' and 'abundance': 0.1914733648300171\n",
      "Cosine Similarity between 'absorbed' and 'abundant': 0.01902916096150875\n",
      "Cosine Similarity between 'absorbed' and 'abundantly': 0.2968914806842804\n",
      "Cosine Similarity between 'absorbed' and 'abuse': 0.08478391915559769\n",
      "Cosine Similarity between 'absorbed' and 'abusive': -0.18758408725261688\n",
      "Cosine Similarity between 'absorbed' and 'academic': 0.23419944941997528\n",
      "Cosine Similarity between 'absorbed' and 'academy': 0.1411716639995575\n",
      "Cosine Similarity between 'absorber' and 'absorbing': -0.19013682007789612\n",
      "Cosine Similarity between 'absorber' and 'absorption': 0.020033640787005424\n",
      "Cosine Similarity between 'absorber' and 'absorptive': -0.1383257508277893\n",
      "Cosine Similarity between 'absorber' and 'abstain': -0.28526169061660767\n",
      "Cosine Similarity between 'absorber' and 'abstract': 0.2956824004650116\n",
      "Cosine Similarity between 'absorber' and 'abstracted': 0.14872877299785614\n",
      "Cosine Similarity between 'absorber' and 'abstraction': 0.12489757686853409\n",
      "Cosine Similarity between 'absorber' and 'abstractly': 0.27117839455604553\n",
      "Cosine Similarity between 'absorber' and 'abu': -0.014907445758581161\n",
      "Cosine Similarity between 'absorber' and 'abundance': -0.006303655914962292\n",
      "Cosine Similarity between 'absorber' and 'abundant': 0.19750334322452545\n",
      "Cosine Similarity between 'absorber' and 'abundantly': -0.1099235787987709\n",
      "Cosine Similarity between 'absorber' and 'abuse': 0.27936968207359314\n",
      "Cosine Similarity between 'absorber' and 'abusive': 0.21237510442733765\n",
      "Cosine Similarity between 'absorber' and 'academic': 0.3469659984111786\n",
      "Cosine Similarity between 'absorber' and 'academy': 0.17662371695041656\n",
      "Cosine Similarity between 'absorbing' and 'absorption': 0.19965142011642456\n",
      "Cosine Similarity between 'absorbing' and 'absorptive': 0.2346678525209427\n",
      "Cosine Similarity between 'absorbing' and 'abstain': 0.3578653931617737\n",
      "Cosine Similarity between 'absorbing' and 'abstract': -0.05544676631689072\n",
      "Cosine Similarity between 'absorbing' and 'abstracted': -0.0677228718996048\n",
      "Cosine Similarity between 'absorbing' and 'abstraction': 0.19227588176727295\n",
      "Cosine Similarity between 'absorbing' and 'abstractly': 0.3607746362686157\n",
      "Cosine Similarity between 'absorbing' and 'abu': 0.2871154248714447\n",
      "Cosine Similarity between 'absorbing' and 'abundance': -0.19953350722789764\n",
      "Cosine Similarity between 'absorbing' and 'abundant': -0.23476579785346985\n",
      "Cosine Similarity between 'absorbing' and 'abundantly': -0.24146446585655212\n",
      "Cosine Similarity between 'absorbing' and 'abuse': -0.3047918379306793\n",
      "Cosine Similarity between 'absorbing' and 'abusive': -0.17019647359848022\n",
      "Cosine Similarity between 'absorbing' and 'academic': -0.3507044017314911\n",
      "Cosine Similarity between 'absorbing' and 'academy': 0.024249542504549026\n",
      "Cosine Similarity between 'absorption' and 'absorptive': 0.0050864745862782\n",
      "Cosine Similarity between 'absorption' and 'abstain': -0.2530892789363861\n",
      "Cosine Similarity between 'absorption' and 'abstract': 0.26981931924819946\n",
      "Cosine Similarity between 'absorption' and 'abstracted': 0.043840792030096054\n",
      "Cosine Similarity between 'absorption' and 'abstraction': 0.35245800018310547\n",
      "Cosine Similarity between 'absorption' and 'abstractly': 0.31772345304489136\n",
      "Cosine Similarity between 'absorption' and 'abu': 0.25046011805534363\n",
      "Cosine Similarity between 'absorption' and 'abundance': 0.368160218000412\n",
      "Cosine Similarity between 'absorption' and 'abundant': 0.10731195658445358\n",
      "Cosine Similarity between 'absorption' and 'abundantly': 0.029743347316980362\n",
      "Cosine Similarity between 'absorption' and 'abuse': -0.04447057843208313\n",
      "Cosine Similarity between 'absorption' and 'abusive': -0.2719881236553192\n",
      "Cosine Similarity between 'absorption' and 'academic': -0.05193479731678963\n",
      "Cosine Similarity between 'absorption' and 'academy': -0.16760693490505219\n",
      "Cosine Similarity between 'absorptive' and 'abstain': 0.025660768151283264\n",
      "Cosine Similarity between 'absorptive' and 'abstract': -0.02508137933909893\n",
      "Cosine Similarity between 'absorptive' and 'abstracted': -0.15010809898376465\n",
      "Cosine Similarity between 'absorptive' and 'abstraction': -0.07431387901306152\n",
      "Cosine Similarity between 'absorptive' and 'abstractly': -0.38468611240386963\n",
      "Cosine Similarity between 'absorptive' and 'abu': -0.5247617363929749\n",
      "Cosine Similarity between 'absorptive' and 'abundance': -0.06070096790790558\n",
      "Cosine Similarity between 'absorptive' and 'abundant': -0.07981124520301819\n",
      "Cosine Similarity between 'absorptive' and 'abundantly': -0.15767890214920044\n",
      "Cosine Similarity between 'absorptive' and 'abuse': -0.27602332830429077\n",
      "Cosine Similarity between 'absorptive' and 'abusive': 0.29146891832351685\n",
      "Cosine Similarity between 'absorptive' and 'academic': 0.058762408792972565\n",
      "Cosine Similarity between 'absorptive' and 'academy': -0.3723433315753937\n",
      "Cosine Similarity between 'abstain' and 'abstract': -0.22213785350322723\n",
      "Cosine Similarity between 'abstain' and 'abstracted': -0.28101757168769836\n",
      "Cosine Similarity between 'abstain' and 'abstraction': -0.1690898835659027\n",
      "Cosine Similarity between 'abstain' and 'abstractly': 0.012367846444249153\n",
      "Cosine Similarity between 'abstain' and 'abu': 0.07289472967386246\n",
      "Cosine Similarity between 'abstain' and 'abundance': 0.0577365979552269\n",
      "Cosine Similarity between 'abstain' and 'abundant': -0.21628092229366302\n",
      "Cosine Similarity between 'abstain' and 'abundantly': -0.38454240560531616\n",
      "Cosine Similarity between 'abstain' and 'abuse': -0.06982863694429398\n",
      "Cosine Similarity between 'abstain' and 'abusive': 0.2199673056602478\n",
      "Cosine Similarity between 'abstain' and 'academic': -0.3555516004562378\n",
      "Cosine Similarity between 'abstain' and 'academy': -0.16873976588249207\n",
      "Cosine Similarity between 'abstract' and 'abstracted': 0.2936181426048279\n",
      "Cosine Similarity between 'abstract' and 'abstraction': 0.18929491937160492\n",
      "Cosine Similarity between 'abstract' and 'abstractly': 0.09540865570306778\n",
      "Cosine Similarity between 'abstract' and 'abu': 0.04524315893650055\n",
      "Cosine Similarity between 'abstract' and 'abundance': -0.1284082531929016\n",
      "Cosine Similarity between 'abstract' and 'abundant': 0.5344817042350769\n",
      "Cosine Similarity between 'abstract' and 'abundantly': -0.20819991827011108\n",
      "Cosine Similarity between 'abstract' and 'abuse': -0.05533872917294502\n",
      "Cosine Similarity between 'abstract' and 'abusive': -0.19641748070716858\n",
      "Cosine Similarity between 'abstract' and 'academic': 0.03739118203520775\n",
      "Cosine Similarity between 'abstract' and 'academy': 0.12958739697933197\n",
      "Cosine Similarity between 'abstracted' and 'abstraction': 0.08424434065818787\n",
      "Cosine Similarity between 'abstracted' and 'abstractly': -0.11328018456697464\n",
      "Cosine Similarity between 'abstracted' and 'abu': 0.20815585553646088\n",
      "Cosine Similarity between 'abstracted' and 'abundance': -0.19165612757205963\n",
      "Cosine Similarity between 'abstracted' and 'abundant': 0.2077062875032425\n",
      "Cosine Similarity between 'abstracted' and 'abundantly': 0.19045405089855194\n",
      "Cosine Similarity between 'abstracted' and 'abuse': -0.2003413885831833\n",
      "Cosine Similarity between 'abstracted' and 'abusive': 0.017036939039826393\n",
      "Cosine Similarity between 'abstracted' and 'academic': 0.023114945739507675\n",
      "Cosine Similarity between 'abstracted' and 'academy': 0.14165151119232178\n",
      "Cosine Similarity between 'abstraction' and 'abstractly': 0.06197117269039154\n",
      "Cosine Similarity between 'abstraction' and 'abu': 0.07045795768499374\n",
      "Cosine Similarity between 'abstraction' and 'abundance': 0.22355180978775024\n",
      "Cosine Similarity between 'abstraction' and 'abundant': 0.053535331040620804\n",
      "Cosine Similarity between 'abstraction' and 'abundantly': -0.17619997262954712\n",
      "Cosine Similarity between 'abstraction' and 'abuse': -0.054901864379644394\n",
      "Cosine Similarity between 'abstraction' and 'abusive': -0.06789606064558029\n",
      "Cosine Similarity between 'abstraction' and 'academic': 0.1783832609653473\n",
      "Cosine Similarity between 'abstraction' and 'academy': -0.06750781089067459\n",
      "Cosine Similarity between 'abstractly' and 'abu': 0.2895813584327698\n",
      "Cosine Similarity between 'abstractly' and 'abundance': -0.019793085753917694\n",
      "Cosine Similarity between 'abstractly' and 'abundant': -0.10907654464244843\n",
      "Cosine Similarity between 'abstractly' and 'abundantly': -0.0038696718402206898\n",
      "Cosine Similarity between 'abstractly' and 'abuse': 0.4296407699584961\n",
      "Cosine Similarity between 'abstractly' and 'abusive': -0.5126270651817322\n",
      "Cosine Similarity between 'abstractly' and 'academic': 0.11245734989643097\n",
      "Cosine Similarity between 'abstractly' and 'academy': 0.13378378748893738\n",
      "Cosine Similarity between 'abu' and 'abundance': -0.035079553723335266\n",
      "Cosine Similarity between 'abu' and 'abundant': 0.03314078971743584\n",
      "Cosine Similarity between 'abu' and 'abundantly': -0.021009201183915138\n",
      "Cosine Similarity between 'abu' and 'abuse': 0.24901536107063293\n",
      "Cosine Similarity between 'abu' and 'abusive': -0.25470831990242004\n",
      "Cosine Similarity between 'abu' and 'academic': -0.3263486623764038\n",
      "Cosine Similarity between 'abu' and 'academy': 0.156795471906662\n",
      "Cosine Similarity between 'abundance' and 'abundant': -0.22946329414844513\n",
      "Cosine Similarity between 'abundance' and 'abundantly': 0.11434774100780487\n",
      "Cosine Similarity between 'abundance' and 'abuse': 0.1357264518737793\n",
      "Cosine Similarity between 'abundance' and 'abusive': -0.04408638924360275\n",
      "Cosine Similarity between 'abundance' and 'academic': 0.33159589767456055\n",
      "Cosine Similarity between 'abundance' and 'academy': -0.024038514122366905\n",
      "Cosine Similarity between 'abundant' and 'abundantly': 0.18666782975196838\n",
      "Cosine Similarity between 'abundant' and 'abuse': -0.062192484736442566\n",
      "Cosine Similarity between 'abundant' and 'abusive': 0.10171303153038025\n",
      "Cosine Similarity between 'abundant' and 'academic': 0.013464163057506084\n",
      "Cosine Similarity between 'abundant' and 'academy': -0.18423160910606384\n",
      "Cosine Similarity between 'abundantly' and 'abuse': -0.2115165889263153\n",
      "Cosine Similarity between 'abundantly' and 'abusive': -0.3076696991920471\n",
      "Cosine Similarity between 'abundantly' and 'academic': 0.20024511218070984\n",
      "Cosine Similarity between 'abundantly' and 'academy': 0.04158177226781845\n",
      "Cosine Similarity between 'abuse' and 'abusive': -0.012630527839064598\n",
      "Cosine Similarity between 'abuse' and 'academic': 0.36668670177459717\n",
      "Cosine Similarity between 'abuse' and 'academy': -0.08853431046009064\n",
      "Cosine Similarity between 'abusive' and 'academic': 0.10396833717823029\n",
      "Cosine Similarity between 'abusive' and 'academy': -0.14954841136932373\n",
      "Cosine Similarity between 'academic' and 'academy': 0.034867215901613235\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(vector_a, vector_b):\n",
    "    dot_product = tf.reduce_sum(tf.multiply(vector_a, vector_b))\n",
    "    norm_a = tf.norm(vector_a)\n",
    "    norm_b = tf.norm(vector_b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "# Loop through the indices from 30 to 50\n",
    "for index_a in range(30, 51):\n",
    "    for index_b in range(index_a+1, 51):  # This ensures we don't compare the word with itself\n",
    "        similarity_score = cosine_similarity(W1_loaded[index_a], W1_loaded[index_b])\n",
    "        \n",
    "        word_a = unique_words[index_a]\n",
    "        word_b = unique_words[index_b]\n",
    "        \n",
    "        print(f\"Cosine Similarity between '{word_a}' and '{word_b}': {similarity_score.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.second method take target matrix as input matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Assuming you have NLTK's stopwords dataset downloaded\n",
    "# If not, you can download using:\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Load English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords_from_file(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Removing stopwords from each line\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        words = line.split()\n",
    "        cleaned_line = ' '.join([word for word in words if word.lower() not in stop_words])\n",
    "        cleaned_lines.append(cleaned_line)\n",
    "\n",
    "    # Writing cleaned lines to a new file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for line in cleaned_lines:\n",
    "            f.write(line + '\\n')\n",
    "\n",
    "# Usage\n",
    "input_filename = 'processed_output2.txt'\n",
    "output_filename = 'new.txt'\n",
    "remove_stopwords_from_file(input_filename, output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "def process_file(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Removing duplicates while maintaining order\n",
    "    unique_sentences = sorted(set(lines), key=lines.index)\n",
    "    \n",
    "    # Extract all unique words, ensuring lowercase representation\n",
    "    all_words = set()\n",
    "    for sentence in unique_sentences:\n",
    "        for word in sentence.split():\n",
    "            all_words.add(word.lower())\n",
    "\n",
    "    return unique_sentences, sorted(all_words)\n",
    "\n",
    "unique_sentences, unique_words = process_file('new.txt')\n",
    "word_to_index = {word: idx for idx, word in enumerate(unique_words)}\n",
    "\n",
    "num_sentences = len(unique_sentences)\n",
    "num_words = len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29442\n",
      "12070\n"
     ]
    }
   ],
   "source": [
    "print(num_sentences)\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "# Step 1: Calculate IDF for each word in the vocabulary\n",
    "\n",
    "word_sentence_indices = defaultdict(set)\n",
    "\n",
    "for idx, sentence in enumerate(unique_sentences):\n",
    "    for word in set(sentence.split()):  # Using set to avoid counting a word multiple times in the same sentence\n",
    "        word_sentence_indices[word.lower()].add(idx)\n",
    "\n",
    "idf = {}\n",
    "for word, sentence_indices in word_sentence_indices.items():\n",
    "    idf[word] = math.log(len(unique_sentences) / len(sentence_indices))\n",
    "\n",
    "# Step 2: Calculate TF for each word in each sentence based on the context window and compute TF-IDF\n",
    "\n",
    "tf_idf_matrix = lil_matrix((len(unique_sentences), len(unique_words)), dtype=float)\n",
    "\n",
    "context_window_size = 3  # -1, 0, 1\n",
    "\n",
    "for idx, sentence in enumerate(unique_sentences):\n",
    "    word_counts = defaultdict(int)\n",
    "    \n",
    "    # Calculate TF (Term Frequency) with the context window\n",
    "    context_start = max(0, idx - (context_window_size // 2))\n",
    "    context_end = min(len(unique_sentences), idx + (context_window_size // 2) + 1)\n",
    "    \n",
    "    total_words_in_context = 0\n",
    "    for i in range(context_start, context_end):\n",
    "        total_words_in_context += len(unique_sentences[i].split())\n",
    "        for word in unique_sentences[i].split():\n",
    "            word_counts[word.lower()] += 1\n",
    "    \n",
    "\n",
    "\n",
    "    for word, count in word_counts.items():\n",
    "        tf = count / total_words_in_context\n",
    "        tf_idf_matrix[idx, unique_words.index(word)] = tf * idf[word]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_matrix = lil_matrix((num_sentences, num_words), dtype=int)\n",
    "output_matrix = lil_matrix((num_sentences, num_words), dtype=int)\n",
    "\n",
    "context_window = 3\n",
    "\n",
    "for idx, sentence in enumerate(unique_sentences):\n",
    "    # Input matrix\n",
    "    for word in sentence.split():\n",
    "        input_matrix[idx, word_to_index[word.lower()]] = 1\n",
    "\n",
    "    # Output matrix\n",
    "    context_start = max(0, idx - (context_window // 2))\n",
    "    context_end = min(num_sentences, idx + (context_window // 2) + 1)\n",
    "    \n",
    "    for i in range(context_start, context_end):\n",
    "        if i != idx:\n",
    "            for word in unique_sentences[i].split():\n",
    "                output_matrix[idx, word_to_index[word.lower()]] = 1\n",
    "\n",
    "# Converting to CSR format for efficient row-wise operations\n",
    "input_matrix = input_matrix.tocsr()\n",
    "output_matrix = output_matrix.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29442, 12070)\n",
      "(29442, 12070)\n",
      "(29442, 12070)\n"
     ]
    }
   ],
   "source": [
    "print(input_matrix.shape)\n",
    "print(tf_idf_matrix.shape)\n",
    "print(output_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12070\n"
     ]
    }
   ],
   "source": [
    "vocab_size = input_matrix.shape[1]\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "vocab_size = input_matrix.shape[1]\n",
    "num_hiddens = 25\n",
    "\n",
    "W1 = tf.Variable(tf.random.normal(shape=(vocab_size, num_hiddens), mean=0, stddev=0.01))\n",
    "W2 = tf.Variable(tf.random.normal(shape=(num_hiddens, vocab_size), mean=0, stddev=0.01))\n",
    "\n",
    "params = [W1, W2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12070, 25)\n"
     ]
    }
   ],
   "source": [
    "print(W1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.398473, shape=(), dtype=float32)\n",
      "tf.Tensor(9.398473, shape=(), dtype=float32)\n",
      "tf.Tensor(9.398472, shape=(), dtype=float32)\n",
      "tf.Tensor(9.398472, shape=(), dtype=float32)\n",
      "tf.Tensor(9.398472, shape=(), dtype=float32)\n",
      "tf.Tensor(9.398472, shape=(), dtype=float32)\n",
      "tf.Tensor(9.398472, shape=(), dtype=float32)\n",
      "tf.Tensor(9.398472, shape=(), dtype=float32)\n",
      "tf.Tensor(9.398472, shape=(), dtype=float32)\n",
      "tf.Tensor(9.398472, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from scipy.sparse import lil_matrix\n",
    "import random\n",
    "\n",
    "# ...[Your previous function and data preparation definitions]\n",
    "\n",
    "def net(X):\n",
    "    X = tf.reshape(tf.cast(X, dtype=tf.float32), (-1, vocab_size))\n",
    "    a1 = tf.matmul(X, W1)  # Replaced input_matrix with X\n",
    "    a2 = tf.matmul(a1, W2)\n",
    "    return softmax(a2)\n",
    "\n",
    "def negative_log_likelihood(y_hat, y):\n",
    "    # Compute the negative log likelihood for true class predictions\n",
    "    return -tf.reduce_mean(tf.math.log(tf.boolean_mask(y_hat, y) + 1e-10))\n",
    "\n",
    "def sgd(params, grads, lr):\n",
    "    \"\"\"Gradient descent.\"\"\"\n",
    "    for param, grad in zip(params, grads):\n",
    "        param.assign_sub(lr * grad)\n",
    "\n",
    "def softmax(X):\n",
    "    X_exp = tf.exp(X)\n",
    "    partition = tf.reduce_sum(X_exp, 1, keepdims=True)\n",
    "    return X_exp / partition \n",
    "\n",
    "# ...[Your previous data loading and processing definitions]\n",
    "\n",
    "# Converting sparse matrices to dense tensors for TensorFlow operations\n",
    "input_tensor = tf.convert_to_tensor(tf_idf_matrix.toarray(), dtype=tf.float32)\n",
    "output_tensor = tf.convert_to_tensor(output_matrix.toarray(), dtype=tf.float32)\n",
    "\n",
    "lr = 0.1\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_hat = net(input_tensor)\n",
    "        l = negative_log_likelihood(y_hat, output_tensor)\n",
    "        print(l)\n",
    "    grads = tape.gradient(l, params)\n",
    "    sgd(params, grads, lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_weight_embeddings\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_weight_embeddings\\assets\n"
     ]
    }
   ],
   "source": [
    "class Embeddings(tf.Module):\n",
    "    def __init__(self, W1, W2):\n",
    "        self.W1 = tf.Variable(W1)\n",
    "        self.W2 = tf.Variable(W2)\n",
    "\n",
    "embedding_module = Embeddings(W1, W2)\n",
    "saved_model_path2 = \"./saved_weight_embeddings\"\n",
    "tf.saved_model.save(embedding_module, saved_model_path2)\n",
    "loaded_module = tf.saved_model.load(saved_model_path2)\n",
    "W1_loaded = loaded_module.W1\n",
    "W2_loaded = loaded_module.W2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between the sentences: 0.6596184372901917\n"
     ]
    }
   ],
   "source": [
    "def sentence_embedding(sentence, W1, word_to_index):\n",
    "    words = sentence.split()\n",
    "    word_indices = [word_to_index[word] for word in words if word in word_to_index]\n",
    "    embeddings = tf.gather(W1, word_indices)\n",
    "    sentence_emb = tf.reduce_mean(embeddings, axis=0)\n",
    "    return sentence_emb\n",
    "\n",
    "def cosine_similarity(vector_a, vector_b):\n",
    "    dot_product = tf.reduce_sum(tf.multiply(vector_a, vector_b))\n",
    "    norm_a = tf.norm(vector_a)\n",
    "    norm_b = tf.norm(vector_b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "sentence_1 = \"Your first test sentence here.\"\n",
    "sentence_2 = \"Your second test sentence here.\"\n",
    "\n",
    "embedding_1 = sentence_embedding(sentence_1, W1_loaded, word_to_index)\n",
    "embedding_2 = sentence_embedding(sentence_2, W1_loaded, word_to_index)\n",
    "\n",
    "similarity_score = cosine_similarity(embedding_1, embedding_2)\n",
    "\n",
    "print(f\"Cosine Similarity between the sentences: {similarity_score.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between Sentence 1 and Sentence 2: 0.000000\n",
      "Cosine Similarity between Sentence 1 and Sentence 3: 0.123091\n",
      "Cosine Similarity between Sentence 1 and Sentence 4: 0.000000\n",
      "Cosine Similarity between Sentence 1 and Sentence 5: 0.000000\n",
      "Cosine Similarity between Sentence 2 and Sentence 3: 0.000000\n",
      "Cosine Similarity between Sentence 2 and Sentence 4: 0.000000\n",
      "Cosine Similarity between Sentence 2 and Sentence 5: 0.000000\n",
      "Cosine Similarity between Sentence 3 and Sentence 4: 0.000000\n",
      "Cosine Similarity between Sentence 3 and Sentence 5: 0.000000\n",
      "Cosine Similarity between Sentence 4 and Sentence 5: 0.080845\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Extract sentence vectors from 1 to 5 from the input_matrix\n",
    "sentence_vectors = input_matrix[1:6].toarray()\n",
    "\n",
    "# Compute cosine similarity between these vectors\n",
    "similarity_matrix = cosine_similarity(sentence_vectors)\n",
    "\n",
    "# Print similarity scores\n",
    "for i in range(5):\n",
    "    for j in range(i+1, 5):\n",
    "        print(f\"Cosine Similarity between Sentence {i+1} and Sentence {j+1}: {similarity_matrix[i][j]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between 'absolutely' and 'absorb': 0.031356293708086014\n",
      "Cosine Similarity between 'absolutely' and 'absorbable': -0.15239395201206207\n",
      "Cosine Similarity between 'absolutely' and 'absorbed': 0.19349133968353271\n",
      "Cosine Similarity between 'absolutely' and 'absorber': -0.07062120735645294\n",
      "Cosine Similarity between 'absolutely' and 'absorbing': -0.3145138919353485\n",
      "Cosine Similarity between 'absolutely' and 'absorption': 0.19709327816963196\n",
      "Cosine Similarity between 'absolutely' and 'absorptive': 0.040041469037532806\n",
      "Cosine Similarity between 'absolutely' and 'abstain': -0.2023737132549286\n",
      "Cosine Similarity between 'absolutely' and 'abstract': -0.11921528726816177\n",
      "Cosine Similarity between 'absolutely' and 'abstracted': -0.022548381239175797\n",
      "Cosine Similarity between 'absolutely' and 'abstraction': 0.23533833026885986\n",
      "Cosine Similarity between 'absolutely' and 'abstractly': 0.07346609979867935\n",
      "Cosine Similarity between 'absolutely' and 'abu': -0.14542092382907867\n",
      "Cosine Similarity between 'absolutely' and 'abundance': -0.1870979368686676\n",
      "Cosine Similarity between 'absolutely' and 'abundant': -0.16063416004180908\n",
      "Cosine Similarity between 'absolutely' and 'abundantly': -0.1358669102191925\n",
      "Cosine Similarity between 'absolutely' and 'abuse': 0.03729987144470215\n",
      "Cosine Similarity between 'absolutely' and 'abusive': 0.018547046929597855\n",
      "Cosine Similarity between 'absolutely' and 'academic': -0.341309130191803\n",
      "Cosine Similarity between 'absolutely' and 'academy': 0.01739867776632309\n",
      "Cosine Similarity between 'absorb' and 'absorbable': -0.18404529988765717\n",
      "Cosine Similarity between 'absorb' and 'absorbed': -0.09530920535326004\n",
      "Cosine Similarity between 'absorb' and 'absorber': -0.03400501236319542\n",
      "Cosine Similarity between 'absorb' and 'absorbing': 0.3208332657814026\n",
      "Cosine Similarity between 'absorb' and 'absorption': -0.277221143245697\n",
      "Cosine Similarity between 'absorb' and 'absorptive': -0.4451947808265686\n",
      "Cosine Similarity between 'absorb' and 'abstain': 0.02382022887468338\n",
      "Cosine Similarity between 'absorb' and 'abstract': -0.392104834318161\n",
      "Cosine Similarity between 'absorb' and 'abstracted': -0.05759868770837784\n",
      "Cosine Similarity between 'absorb' and 'abstraction': -0.17246274650096893\n",
      "Cosine Similarity between 'absorb' and 'abstractly': -0.17664188146591187\n",
      "Cosine Similarity between 'absorb' and 'abu': 0.025523310527205467\n",
      "Cosine Similarity between 'absorb' and 'abundance': 0.3308323919773102\n",
      "Cosine Similarity between 'absorb' and 'abundant': -0.22376884520053864\n",
      "Cosine Similarity between 'absorb' and 'abundantly': 0.006386423483490944\n",
      "Cosine Similarity between 'absorb' and 'abuse': -0.3553476929664612\n",
      "Cosine Similarity between 'absorb' and 'abusive': -0.08580093830823898\n",
      "Cosine Similarity between 'absorb' and 'academic': 0.17326316237449646\n",
      "Cosine Similarity between 'absorb' and 'academy': -0.0017782270442694426\n",
      "Cosine Similarity between 'absorbable' and 'absorbed': 0.13090918958187103\n",
      "Cosine Similarity between 'absorbable' and 'absorber': 0.001963139045983553\n",
      "Cosine Similarity between 'absorbable' and 'absorbing': -0.21221904456615448\n",
      "Cosine Similarity between 'absorbable' and 'absorption': -0.034241095185279846\n",
      "Cosine Similarity between 'absorbable' and 'absorptive': 0.27368780970573425\n",
      "Cosine Similarity between 'absorbable' and 'abstain': -0.09957654774188995\n",
      "Cosine Similarity between 'absorbable' and 'abstract': 0.28953248262405396\n",
      "Cosine Similarity between 'absorbable' and 'abstracted': 0.0763712227344513\n",
      "Cosine Similarity between 'absorbable' and 'abstraction': -0.2696094214916229\n",
      "Cosine Similarity between 'absorbable' and 'abstractly': -0.06103624403476715\n",
      "Cosine Similarity between 'absorbable' and 'abu': 0.1455484926700592\n",
      "Cosine Similarity between 'absorbable' and 'abundance': -0.047119833528995514\n",
      "Cosine Similarity between 'absorbable' and 'abundant': -0.0008322790963575244\n",
      "Cosine Similarity between 'absorbable' and 'abundantly': -0.1914464235305786\n",
      "Cosine Similarity between 'absorbable' and 'abuse': 0.24697797000408173\n",
      "Cosine Similarity between 'absorbable' and 'abusive': 0.3169984817504883\n",
      "Cosine Similarity between 'absorbable' and 'academic': 0.04955804720520973\n",
      "Cosine Similarity between 'absorbable' and 'academy': 0.3068297505378723\n",
      "Cosine Similarity between 'absorbed' and 'absorber': 0.084615558385849\n",
      "Cosine Similarity between 'absorbed' and 'absorbing': 0.0313236229121685\n",
      "Cosine Similarity between 'absorbed' and 'absorption': 0.4629504382610321\n",
      "Cosine Similarity between 'absorbed' and 'absorptive': 0.16058659553527832\n",
      "Cosine Similarity between 'absorbed' and 'abstain': 0.137131467461586\n",
      "Cosine Similarity between 'absorbed' and 'abstract': 0.09849485754966736\n",
      "Cosine Similarity between 'absorbed' and 'abstracted': 0.04305355250835419\n",
      "Cosine Similarity between 'absorbed' and 'abstraction': -0.3341978192329407\n",
      "Cosine Similarity between 'absorbed' and 'abstractly': 0.007660684175789356\n",
      "Cosine Similarity between 'absorbed' and 'abu': 0.1485695242881775\n",
      "Cosine Similarity between 'absorbed' and 'abundance': -0.2106831818819046\n",
      "Cosine Similarity between 'absorbed' and 'abundant': -0.1131313294172287\n",
      "Cosine Similarity between 'absorbed' and 'abundantly': 0.4310659170150757\n",
      "Cosine Similarity between 'absorbed' and 'abuse': 0.47476431727409363\n",
      "Cosine Similarity between 'absorbed' and 'abusive': -0.26586854457855225\n",
      "Cosine Similarity between 'absorbed' and 'academic': -0.16298742592334747\n",
      "Cosine Similarity between 'absorbed' and 'academy': 0.28109315037727356\n",
      "Cosine Similarity between 'absorber' and 'absorbing': -0.10438529402017593\n",
      "Cosine Similarity between 'absorber' and 'absorption': 0.23036599159240723\n",
      "Cosine Similarity between 'absorber' and 'absorptive': -0.05288562551140785\n",
      "Cosine Similarity between 'absorber' and 'abstain': -0.2537015676498413\n",
      "Cosine Similarity between 'absorber' and 'abstract': 0.1895591914653778\n",
      "Cosine Similarity between 'absorber' and 'abstracted': -0.27660802006721497\n",
      "Cosine Similarity between 'absorber' and 'abstraction': 0.020966066047549248\n",
      "Cosine Similarity between 'absorber' and 'abstractly': 0.1376105546951294\n",
      "Cosine Similarity between 'absorber' and 'abu': 0.16062679886817932\n",
      "Cosine Similarity between 'absorber' and 'abundance': 0.30042293667793274\n",
      "Cosine Similarity between 'absorber' and 'abundant': 0.1887771487236023\n",
      "Cosine Similarity between 'absorber' and 'abundantly': -0.1259741187095642\n",
      "Cosine Similarity between 'absorber' and 'abuse': 0.08613964170217514\n",
      "Cosine Similarity between 'absorber' and 'abusive': -0.3290424048900604\n",
      "Cosine Similarity between 'absorber' and 'academic': 0.1956784725189209\n",
      "Cosine Similarity between 'absorber' and 'academy': 0.01669967547059059\n",
      "Cosine Similarity between 'absorbing' and 'absorption': -0.00742594338953495\n",
      "Cosine Similarity between 'absorbing' and 'absorptive': -0.26699957251548767\n",
      "Cosine Similarity between 'absorbing' and 'abstain': -0.11561901122331619\n",
      "Cosine Similarity between 'absorbing' and 'abstract': -0.2645496428012848\n",
      "Cosine Similarity between 'absorbing' and 'abstracted': -0.11796822398900986\n",
      "Cosine Similarity between 'absorbing' and 'abstraction': 0.05505630746483803\n",
      "Cosine Similarity between 'absorbing' and 'abstractly': -0.2938447892665863\n",
      "Cosine Similarity between 'absorbing' and 'abu': -0.026396017521619797\n",
      "Cosine Similarity between 'absorbing' and 'abundance': -0.31763023138046265\n",
      "Cosine Similarity between 'absorbing' and 'abundant': -0.35950201749801636\n",
      "Cosine Similarity between 'absorbing' and 'abundantly': 0.2522210478782654\n",
      "Cosine Similarity between 'absorbing' and 'abuse': -0.321254700422287\n",
      "Cosine Similarity between 'absorbing' and 'abusive': 0.02262299321591854\n",
      "Cosine Similarity between 'absorbing' and 'academic': 0.4183982014656067\n",
      "Cosine Similarity between 'absorbing' and 'academy': 0.44526728987693787\n",
      "Cosine Similarity between 'absorption' and 'absorptive': 0.11268099397420883\n",
      "Cosine Similarity between 'absorption' and 'abstain': 0.15327292680740356\n",
      "Cosine Similarity between 'absorption' and 'abstract': 0.11327537894248962\n",
      "Cosine Similarity between 'absorption' and 'abstracted': -0.12384335696697235\n",
      "Cosine Similarity between 'absorption' and 'abstraction': -0.003985617309808731\n",
      "Cosine Similarity between 'absorption' and 'abstractly': -0.18323668837547302\n",
      "Cosine Similarity between 'absorption' and 'abu': 0.24648353457450867\n",
      "Cosine Similarity between 'absorption' and 'abundance': -0.09229770302772522\n",
      "Cosine Similarity between 'absorption' and 'abundant': -0.029052384197711945\n",
      "Cosine Similarity between 'absorption' and 'abundantly': 0.00031641917303204536\n",
      "Cosine Similarity between 'absorption' and 'abuse': 0.4353489577770233\n",
      "Cosine Similarity between 'absorption' and 'abusive': -0.10001339763402939\n",
      "Cosine Similarity between 'absorption' and 'academic': -0.02394300512969494\n",
      "Cosine Similarity between 'absorption' and 'academy': 0.3290209472179413\n",
      "Cosine Similarity between 'absorptive' and 'abstain': -0.06473849713802338\n",
      "Cosine Similarity between 'absorptive' and 'abstract': 0.37928909063339233\n",
      "Cosine Similarity between 'absorptive' and 'abstracted': 0.37366417050361633\n",
      "Cosine Similarity between 'absorptive' and 'abstraction': -0.020977776497602463\n",
      "Cosine Similarity between 'absorptive' and 'abstractly': 0.2941148281097412\n",
      "Cosine Similarity between 'absorptive' and 'abu': 0.1501573771238327\n",
      "Cosine Similarity between 'absorptive' and 'abundance': -0.3723311424255371\n",
      "Cosine Similarity between 'absorptive' and 'abundant': -0.06132278963923454\n",
      "Cosine Similarity between 'absorptive' and 'abundantly': -0.10192982852458954\n",
      "Cosine Similarity between 'absorptive' and 'abuse': 0.2769862115383148\n",
      "Cosine Similarity between 'absorptive' and 'abusive': 0.12175517529249191\n",
      "Cosine Similarity between 'absorptive' and 'academic': -0.285925030708313\n",
      "Cosine Similarity between 'absorptive' and 'academy': -0.1059723049402237\n",
      "Cosine Similarity between 'abstain' and 'abstract': -0.05761263519525528\n",
      "Cosine Similarity between 'abstain' and 'abstracted': 0.14515160024166107\n",
      "Cosine Similarity between 'abstain' and 'abstraction': -0.435785174369812\n",
      "Cosine Similarity between 'abstain' and 'abstractly': -0.07774943113327026\n",
      "Cosine Similarity between 'abstain' and 'abu': 0.4072650074958801\n",
      "Cosine Similarity between 'abstain' and 'abundance': 0.3553467392921448\n",
      "Cosine Similarity between 'abstain' and 'abundant': 0.0013675395166501403\n",
      "Cosine Similarity between 'abstain' and 'abundantly': -0.1321226954460144\n",
      "Cosine Similarity between 'abstain' and 'abuse': 0.44227728247642517\n",
      "Cosine Similarity between 'abstain' and 'abusive': -0.06952866911888123\n",
      "Cosine Similarity between 'abstain' and 'academic': -0.18653540313243866\n",
      "Cosine Similarity between 'abstain' and 'academy': 0.1870519369840622\n",
      "Cosine Similarity between 'abstract' and 'abstracted': -0.32024484872817993\n",
      "Cosine Similarity between 'abstract' and 'abstraction': 0.18536148965358734\n",
      "Cosine Similarity between 'abstract' and 'abstractly': 0.1379728764295578\n",
      "Cosine Similarity between 'abstract' and 'abu': 0.17146530747413635\n",
      "Cosine Similarity between 'abstract' and 'abundance': -0.06280574947595596\n",
      "Cosine Similarity between 'abstract' and 'abundant': 0.2533067762851715\n",
      "Cosine Similarity between 'abstract' and 'abundantly': 0.14648188650608063\n",
      "Cosine Similarity between 'abstract' and 'abuse': -0.11578352749347687\n",
      "Cosine Similarity between 'abstract' and 'abusive': -0.0038841005880385637\n",
      "Cosine Similarity between 'abstract' and 'academic': 0.0443788506090641\n",
      "Cosine Similarity between 'abstract' and 'academy': -0.08860602229833603\n",
      "Cosine Similarity between 'abstracted' and 'abstraction': -0.3253777027130127\n",
      "Cosine Similarity between 'abstracted' and 'abstractly': -0.1802147924900055\n",
      "Cosine Similarity between 'abstracted' and 'abu': 0.06412387639284134\n",
      "Cosine Similarity between 'abstracted' and 'abundance': -0.16213443875312805\n",
      "Cosine Similarity between 'abstracted' and 'abundant': -0.23819425702095032\n",
      "Cosine Similarity between 'abstracted' and 'abundantly': -0.10890312492847443\n",
      "Cosine Similarity between 'abstracted' and 'abuse': 0.291960209608078\n",
      "Cosine Similarity between 'abstracted' and 'abusive': 0.13961146771907806\n",
      "Cosine Similarity between 'abstracted' and 'academic': -0.45829498767852783\n",
      "Cosine Similarity between 'abstracted' and 'academy': -0.02155282162129879\n",
      "Cosine Similarity between 'abstraction' and 'abstractly': 0.001998292747884989\n",
      "Cosine Similarity between 'abstraction' and 'abu': -0.24723324179649353\n",
      "Cosine Similarity between 'abstraction' and 'abundance': -0.2424595057964325\n",
      "Cosine Similarity between 'abstraction' and 'abundant': 0.06133175268769264\n",
      "Cosine Similarity between 'abstraction' and 'abundantly': 0.020275229588150978\n",
      "Cosine Similarity between 'abstraction' and 'abuse': -0.41390401124954224\n",
      "Cosine Similarity between 'abstraction' and 'abusive': 0.23824656009674072\n",
      "Cosine Similarity between 'abstraction' and 'academic': 0.22366046905517578\n",
      "Cosine Similarity between 'abstraction' and 'academy': -0.15648618340492249\n",
      "Cosine Similarity between 'abstractly' and 'abu': -0.3701736032962799\n",
      "Cosine Similarity between 'abstractly' and 'abundance': 0.024451375007629395\n",
      "Cosine Similarity between 'abstractly' and 'abundant': 0.12292028963565826\n",
      "Cosine Similarity between 'abstractly' and 'abundantly': -0.19482123851776123\n",
      "Cosine Similarity between 'abstractly' and 'abuse': 0.1675865203142166\n",
      "Cosine Similarity between 'abstractly' and 'abusive': -0.3201843202114105\n",
      "Cosine Similarity between 'abstractly' and 'academic': 0.07936563342809677\n",
      "Cosine Similarity between 'abstractly' and 'academy': -0.5295780301094055\n",
      "Cosine Similarity between 'abu' and 'abundance': 0.19264498353004456\n",
      "Cosine Similarity between 'abu' and 'abundant': -0.1727037876844406\n",
      "Cosine Similarity between 'abu' and 'abundantly': 0.025008799508213997\n",
      "Cosine Similarity between 'abu' and 'abuse': 0.05857022851705551\n",
      "Cosine Similarity between 'abu' and 'abusive': -0.044748999178409576\n",
      "Cosine Similarity between 'abu' and 'academic': -0.019421018660068512\n",
      "Cosine Similarity between 'abu' and 'academy': 0.510057806968689\n",
      "Cosine Similarity between 'abundance' and 'abundant': 0.1955132633447647\n",
      "Cosine Similarity between 'abundance' and 'abundantly': -0.08232450485229492\n",
      "Cosine Similarity between 'abundance' and 'abuse': -0.07602041214704514\n",
      "Cosine Similarity between 'abundance' and 'abusive': -0.05049829185009003\n",
      "Cosine Similarity between 'abundance' and 'academic': 0.22650542855262756\n",
      "Cosine Similarity between 'abundance' and 'academy': -0.2272220104932785\n",
      "Cosine Similarity between 'abundant' and 'abundantly': -0.02792493812739849\n",
      "Cosine Similarity between 'abundant' and 'abuse': 0.18639351427555084\n",
      "Cosine Similarity between 'abundant' and 'abusive': 0.05214384198188782\n",
      "Cosine Similarity between 'abundant' and 'academic': -0.2066357135772705\n",
      "Cosine Similarity between 'abundant' and 'academy': -0.29982414841651917\n",
      "Cosine Similarity between 'abundantly' and 'abuse': -0.22926467657089233\n",
      "Cosine Similarity between 'abundantly' and 'abusive': -0.1105257049202919\n",
      "Cosine Similarity between 'abundantly' and 'academic': 0.09657036513090134\n",
      "Cosine Similarity between 'abundantly' and 'academy': -0.04220627248287201\n",
      "Cosine Similarity between 'abuse' and 'abusive': -0.19403114914894104\n",
      "Cosine Similarity between 'abuse' and 'academic': -0.3436242640018463\n",
      "Cosine Similarity between 'abuse' and 'academy': 0.17781206965446472\n",
      "Cosine Similarity between 'abusive' and 'academic': -0.058255068957805634\n",
      "Cosine Similarity between 'abusive' and 'academy': 0.17486606538295746\n",
      "Cosine Similarity between 'academic' and 'academy': 0.09443721920251846\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(vector_a, vector_b):\n",
    "    dot_product = tf.reduce_sum(tf.multiply(vector_a, vector_b))\n",
    "    norm_a = tf.norm(vector_a)\n",
    "    norm_b = tf.norm(vector_b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "# Loop through the indices from 30 to 50\n",
    "for index_a in range(30, 51):\n",
    "    for index_b in range(index_a+1, 51):  # This ensures we don't compare the word with itself\n",
    "        similarity_score = cosine_similarity(W1_loaded[index_a], W1_loaded[index_b])\n",
    "        \n",
    "        word_a = unique_words[index_a]\n",
    "        word_b = unique_words[index_b]\n",
    "        \n",
    "        print(f\"Cosine Similarity between '{word_a}' and '{word_b}': {similarity_score.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Removing duplicates while maintaining order\n",
    "    unique_sentences2 = sorted(set(lines), key=lines.index)\n",
    "    return unique_sentences2\n",
    "\n",
    "test_sentences = process_file('shraddha8.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(sentence, W1, vocab):\n",
    "    words = sentence.split()\n",
    "    embedding_sum = None\n",
    "    valid_word_count = 0\n",
    "    for word in words:\n",
    "        if word in vocab:  # only consider words that have an embedding\n",
    "            word_idx = vocab[word]\n",
    "            word_embedding = W1[word_idx].numpy()\n",
    "            if embedding_sum is None:\n",
    "                embedding_sum = word_embedding\n",
    "            else:\n",
    "                embedding_sum += word_embedding\n",
    "            valid_word_count += 1\n",
    "\n",
    "    # Compute the average embedding\n",
    "    if valid_word_count == 0:\n",
    "        return None  # or you could return a zero vector of embedding size\n",
    "    avg_embedding = embedding_sum / valid_word_count\n",
    "    return avg_embedding\n",
    "\n",
    "def cosine_similarity(vector_a, vector_b):\n",
    "    dot_product = tf.reduce_sum(tf.multiply(vector_a, vector_b))\n",
    "    norm_a = tf.norm(vector_a)\n",
    "    norm_b = tf.norm(vector_b)\n",
    "    return dot_product / (norm_a * norm_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between sentences: -0.2759794294834137\n"
     ]
    }
   ],
   "source": [
    "sentence_1 = test_sentences[1]  # Example sentence index\n",
    "sentence_2 = test_sentences[5]  # Another example sentence index\n",
    "\n",
    "embedding_1 = get_sentence_embedding(sentence_1, W1_loaded, word_to_index)\n",
    "embedding_2 = get_sentence_embedding(sentence_2, W1_loaded, word_to_index)\n",
    "\n",
    "similarity = cosine_similarity(embedding_1, embedding_2)\n",
    "print(f\"Cosine Similarity between sentences: {similarity.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 30.00%\n"
     ]
    }
   ],
   "source": [
    "def process_file(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Removing duplicates while maintaining order\n",
    "    unique_sentences = sorted(set(lines), key=lines.index)\n",
    "    \n",
    "    # Extract all unique words, ensuring lowercase representation\n",
    "    all_words = set()\n",
    "    for sentence in unique_sentences:\n",
    "        for word in sentence.split():\n",
    "            all_words.add(word.lower())\n",
    "\n",
    "    return unique_sentences, sorted(all_words)\n",
    "\n",
    "unique_sentences, unique_words = process_file('new.txt')\n",
    "original_vocab = {word: idx for idx, word in enumerate(unique_words)}\n",
    "\n",
    "# ... [Rest of the code to load the model, W1_loaded, etc.]\n",
    "\n",
    "test_sentences, _ = process_file('shraddha8.txt')\n",
    "\n",
    "accuracy_count = 0\n",
    "threshold = 0.1\n",
    "\n",
    "# Compare only the first 100 sentences as per your request\n",
    "for i in range(0, 100, 10):\n",
    "    sentence_a = test_sentences[i]\n",
    "    sentence_b = test_sentences[i+1]\n",
    "    \n",
    "    embedding_a = sentence_embedding(sentence_a, W1_loaded.numpy(), original_vocab)\n",
    "    embedding_b = sentence_embedding(sentence_b, W1_loaded.numpy(), original_vocab)\n",
    "    \n",
    "    similarity = cosine_similarity(embedding_a, embedding_b)\n",
    "    if similarity > threshold:\n",
    "        accuracy_count += 1\n",
    "\n",
    "accuracy = accuracy_count / 10\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between sentence 0 and 1: 0.0524\n",
      "Cosine Similarity between sentence 1 and 2: -0.0010\n",
      "Cosine Similarity between sentence 2 and 3: 0.0012\n",
      "Cosine Similarity between sentence 3 and 4: 0.0347\n",
      "Cosine Similarity between sentence 4 and 5: 0.0345\n",
      "Cosine Similarity between sentence 5 and 6: -0.2966\n",
      "Cosine Similarity between sentence 6 and 7: 0.4724\n",
      "Cosine Similarity between sentence 7 and 8: 0.2890\n",
      "Cosine Similarity between sentence 8 and 9: 0.0036\n",
      "Cosine Similarity between sentence 9 and 10: -0.1691\n",
      "Cosine Similarity between sentence 10 and 11: 0.3095\n",
      "Cosine Similarity between sentence 11 and 12: 0.3289\n",
      "Cosine Similarity between sentence 12 and 13: 0.2594\n",
      "Cosine Similarity between sentence 13 and 14: 0.1393\n",
      "Cosine Similarity between sentence 14 and 15: 0.1190\n",
      "Cosine Similarity between sentence 15 and 16: -0.0497\n",
      "Cosine Similarity between sentence 16 and 17: 0.4294\n",
      "Cosine Similarity between sentence 17 and 18: 0.5102\n",
      "Cosine Similarity between sentence 18 and 19: -0.2344\n",
      "Cosine Similarity between sentence 19 and 20: 0.3532\n",
      "Cosine Similarity between sentence 20 and 21: -0.0833\n",
      "Cosine Similarity between sentence 21 and 22: 0.2827\n",
      "Cosine Similarity between sentence 22 and 23: 0.3400\n",
      "Cosine Similarity between sentence 23 and 24: -0.0896\n",
      "Cosine Similarity between sentence 24 and 25: 0.0565\n",
      "Cosine Similarity between sentence 25 and 26: 0.3789\n",
      "Cosine Similarity between sentence 26 and 27: 0.2618\n",
      "Cosine Similarity between sentence 27 and 28: 0.0154\n",
      "Cosine Similarity between sentence 28 and 29: -0.3332\n",
      "Cosine Similarity between sentence 29 and 30: -0.2100\n",
      "Cosine Similarity between sentence 30 and 31: -0.1422\n",
      "Cosine Similarity between sentence 31 and 32: -0.2870\n",
      "Cosine Similarity between sentence 32 and 33: 0.1261\n",
      "Cosine Similarity between sentence 33 and 34: -0.3201\n",
      "Cosine Similarity between sentence 34 and 35: -0.1028\n",
      "Cosine Similarity between sentence 35 and 36: 0.0512\n",
      "Cosine Similarity between sentence 36 and 37: 0.3540\n",
      "Cosine Similarity between sentence 37 and 38: 0.2280\n",
      "Cosine Similarity between sentence 38 and 39: -0.0682\n",
      "Cosine Similarity between sentence 39 and 40: 0.3858\n",
      "Cosine Similarity between sentence 40 and 41: 0.0131\n",
      "Cosine Similarity between sentence 41 and 42: 0.5275\n",
      "Cosine Similarity between sentence 42 and 43: 0.4402\n",
      "Cosine Similarity between sentence 43 and 44: 0.0144\n",
      "Cosine Similarity between sentence 44 and 45: 0.0374\n",
      "Cosine Similarity between sentence 45 and 46: 0.6143\n",
      "Cosine Similarity between sentence 46 and 47: 0.6308\n",
      "Cosine Similarity between sentence 47 and 48: 0.0469\n",
      "Cosine Similarity between sentence 48 and 49: 0.2468\n",
      "Cosine Similarity between sentence 49 and 50: 0.5012\n",
      "Cosine Similarity between sentence 50 and 51: -0.3072\n",
      "Cosine Similarity between sentence 51 and 52: -0.0648\n",
      "Cosine Similarity between sentence 52 and 53: -0.2710\n",
      "Cosine Similarity between sentence 53 and 54: 0.5473\n",
      "Cosine Similarity between sentence 54 and 55: 0.6313\n",
      "Cosine Similarity between sentence 55 and 56: -0.1293\n",
      "Cosine Similarity between sentence 56 and 57: -0.1015\n",
      "Cosine Similarity between sentence 57 and 58: -0.0884\n",
      "Cosine Similarity between sentence 58 and 59: -0.4229\n",
      "Cosine Similarity between sentence 59 and 60: -0.2287\n",
      "Cosine Similarity between sentence 60 and 61: -0.0447\n",
      "Cosine Similarity between sentence 61 and 62: 0.1219\n",
      "Cosine Similarity between sentence 62 and 63: -0.0219\n",
      "Cosine Similarity between sentence 63 and 64: 0.0412\n",
      "Cosine Similarity between sentence 64 and 65: 0.5019\n",
      "Cosine Similarity between sentence 65 and 66: 0.5250\n",
      "Cosine Similarity between sentence 66 and 67: 0.0790\n",
      "Cosine Similarity between sentence 67 and 68: -0.3935\n",
      "Cosine Similarity between sentence 68 and 69: -0.0248\n",
      "Cosine Similarity between sentence 69 and 70: 0.1642\n",
      "Cosine Similarity between sentence 70 and 71: 0.1616\n",
      "Cosine Similarity between sentence 71 and 72: -0.1231\n",
      "Cosine Similarity between sentence 72 and 73: -0.3417\n",
      "Cosine Similarity between sentence 73 and 74: -0.1854\n",
      "Cosine Similarity between sentence 74 and 75: 0.3474\n",
      "Cosine Similarity between sentence 75 and 76: 0.0310\n",
      "Cosine Similarity between sentence 76 and 77: 0.0897\n",
      "Cosine Similarity between sentence 77 and 78: -0.0548\n",
      "Cosine Similarity between sentence 78 and 79: 0.1548\n",
      "Cosine Similarity between sentence 79 and 80: 0.2741\n",
      "Cosine Similarity between sentence 80 and 81: 0.3281\n",
      "Cosine Similarity between sentence 81 and 82: 0.0135\n",
      "Cosine Similarity between sentence 82 and 83: -0.1155\n",
      "Cosine Similarity between sentence 83 and 84: -0.1529\n",
      "Cosine Similarity between sentence 84 and 85: 0.3290\n",
      "Cosine Similarity between sentence 85 and 86: 0.1078\n",
      "Cosine Similarity between sentence 86 and 87: 0.3988\n",
      "Cosine Similarity between sentence 87 and 88: -0.1428\n",
      "Cosine Similarity between sentence 88 and 89: 0.3406\n",
      "Cosine Similarity between sentence 89 and 90: 0.3928\n",
      "Cosine Similarity between sentence 90 and 91: -0.0582\n",
      "Cosine Similarity between sentence 91 and 92: -0.1083\n",
      "Cosine Similarity between sentence 92 and 93: 0.4390\n",
      "Cosine Similarity between sentence 93 and 94: 0.7213\n",
      "Cosine Similarity between sentence 94 and 95: 0.3052\n",
      "Cosine Similarity between sentence 95 and 96: 0.1365\n",
      "Cosine Similarity between sentence 96 and 97: 0.4699\n",
      "Cosine Similarity between sentence 97 and 98: 0.3721\n",
      "Cosine Similarity between sentence 98 and 99: 0.2980\n"
     ]
    }
   ],
   "source": [
    "for i in range(99):  # We'll loop till 99 so that i+1 for the last sentence doesn't cause an index error\n",
    "    sentence_a = test_sentences[i]\n",
    "    sentence_b = test_sentences[i+1]\n",
    "    \n",
    "    embedding_a = sentence_embedding(sentence_a, W1_loaded.numpy(), original_vocab)\n",
    "    embedding_b = sentence_embedding(sentence_b, W1_loaded.numpy(), original_vocab)\n",
    "    \n",
    "    similarity = cosine_similarity(embedding_a, embedding_b)\n",
    "    print(f\"Cosine Similarity between sentence {i} and {i+1}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "third method -- we can back progate and update with help of vector instaed of whole matrix but for my training daatset this is comptutationally expensive (this is for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Word2VecSkipgram:\n",
    "    def __init__(self, embedding_dim, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.W_input = None  # Input (target) matrix\n",
    "        self.W_output = None  # Output (context) matrix\n",
    "\n",
    "    def initialize_weights(self, vocab_size):\n",
    "        \"\"\"\n",
    "        Initialize weights with random values\n",
    "        \"\"\"\n",
    "        self.W_input = np.random.rand(vocab_size, self.embedding_dim)\n",
    "        self.W_output = np.random.rand(self.embedding_dim, vocab_size)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Compute the softmax of vector x.\n",
    "        \"\"\"\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "\n",
    "    def forward_pass(self, input_vector):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "        \"\"\"\n",
    "        h = np.dot(self.W_input.T, input_vector)\n",
    "        u = np.dot(self.W_output.T, h)\n",
    "        y_pred = self.softmax(u)\n",
    "        return y_pred, h, u\n",
    "\n",
    "    def backward_pass(self, input_vector, h, u, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Backward propagation\n",
    "        \"\"\"\n",
    "        # Compute the error\n",
    "        e = y_pred - y_true\n",
    "        dW_output = np.outer(h, e)\n",
    "        dW_input = np.outer(input_vector, np.dot(self.W_output, e))\n",
    "\n",
    "        # Update weights\n",
    "        self.W_output -= self.learning_rate * dW_output\n",
    "        self.W_input -= self.learning_rate * dW_input\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute the loss using cross-entropy\n",
    "        \"\"\"\n",
    "        l = -np.sum(np.log(y_pred) * y_true)\n",
    "        return l\n",
    "\n",
    "    def train(self, input_matrix, output_matrix,epochs):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "        \"\"\"\n",
    "        # Initialize weights\n",
    "        vocab_size = input_matrix.shape[1]\n",
    "        self.initialize_weights(vocab_size)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for i, vec in enumerate(input_matrix):\n",
    "                y_true = output_matrix[i]\n",
    "                input_vector = vec.T\n",
    "                y_pred, h, u = self.forward_pass(input_vector)\n",
    "                self.backward_pass(input_vector, h, u, y_pred, output_matrix[i])\n",
    "                epoch_loss += self.compute_loss(output_matrix[i], y_pred)\n",
    "            print(f\"Epoch {epoch + 1}/{epochs} - Loss: {epoch_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 - Loss: 3.0829096226191\n",
      "Epoch 2/1000 - Loss: 3.0713532792541844\n",
      "Epoch 3/1000 - Loss: 3.0599642909220717\n",
      "Epoch 4/1000 - Loss: 3.0487363190761467\n",
      "Epoch 5/1000 - Loss: 3.0376632082660366\n",
      "Epoch 6/1000 - Loss: 3.0267389795814115\n",
      "Epoch 7/1000 - Loss: 3.0159578243618514\n",
      "Epoch 8/1000 - Loss: 3.0053140981633684\n",
      "Epoch 9/1000 - Loss: 2.994802314972519\n",
      "Epoch 10/1000 - Loss: 2.9844171416593275\n",
      "Epoch 11/1000 - Loss: 2.9741533926605785\n",
      "Epoch 12/1000 - Loss: 2.9640060248853204\n",
      "Epoch 13/1000 - Loss: 2.953970132834721\n",
      "Epoch 14/1000 - Loss: 2.9440409439287\n",
      "Epoch 15/1000 - Loss: 2.934213814032044\n",
      "Epoch 16/1000 - Loss: 2.924484223172986\n",
      "Epoch 17/1000 - Loss: 2.914847771447467\n",
      "Epoch 18/1000 - Loss: 2.905300175102594\n",
      "Epoch 19/1000 - Loss: 2.895837262793013\n",
      "Epoch 20/1000 - Loss: 2.886454972004173\n",
      "Epoch 21/1000 - Loss: 2.877149345636693\n",
      "Epoch 22/1000 - Loss: 2.8679165287462363\n",
      "Epoch 23/1000 - Loss: 2.8587527654335383\n",
      "Epoch 24/1000 - Loss: 2.8496543958794205\n",
      "Epoch 25/1000 - Loss: 2.8406178535198188\n",
      "Epoch 26/1000 - Loss: 2.831639662356051\n",
      "Epoch 27/1000 - Loss: 2.8227164343957085\n",
      "Epoch 28/1000 - Loss: 2.813844867219753\n",
      "Epoch 29/1000 - Loss: 2.805021741671527\n",
      "Epoch 30/1000 - Loss: 2.79624391966358\n",
      "Epoch 31/1000 - Loss: 2.7875083420983175\n",
      "Epoch 32/1000 - Loss: 2.7788120268986525\n",
      "Epoch 33/1000 - Loss: 2.7701520671449424\n",
      "Epoch 34/1000 - Loss: 2.7615256293146344\n",
      "Epoch 35/1000 - Loss: 2.7529299516211436\n",
      "Epoch 36/1000 - Loss: 2.744362342448609\n",
      "Epoch 37/1000 - Loss: 2.7358201788792496\n",
      "Epoch 38/1000 - Loss: 2.7273009053101624\n",
      "Epoch 39/1000 - Loss: 2.718802032156466\n",
      "Epoch 40/1000 - Loss: 2.7103211346377813\n",
      "Epoch 41/1000 - Loss: 2.701855851645109\n",
      "Epoch 42/1000 - Loss: 2.693403884685228\n",
      "Epoch 43/1000 - Loss: 2.6849629968997926\n",
      "Epoch 44/1000 - Loss: 2.6765310121563566\n",
      "Epoch 45/1000 - Loss: 2.668105814208608\n",
      "Epoch 46/1000 - Loss: 2.6596853459231093\n",
      "Epoch 47/1000 - Loss: 2.651267608569893\n",
      "Epoch 48/1000 - Loss: 2.642850661174281\n",
      "Epoch 49/1000 - Loss: 2.6344326199273076\n",
      "Epoch 50/1000 - Loss: 2.6260116576521444\n",
      "Epoch 51/1000 - Loss: 2.6175860033239404\n",
      "Epoch 52/1000 - Loss: 2.609153941640476\n",
      "Epoch 53/1000 - Loss: 2.6007138126410476\n",
      "Epoch 54/1000 - Loss: 2.592264011370975\n",
      "Epoch 55/1000 - Loss: 2.5838029875891255\n",
      "Epoch 56/1000 - Loss: 2.5753292455158183\n",
      "Epoch 57/1000 - Loss: 2.5668413436184743\n",
      "Epoch 58/1000 - Loss: 2.558337894432323\n",
      "Epoch 59/1000 - Loss: 2.5498175644134893\n",
      "Epoch 60/1000 - Loss: 2.5412790738217135\n",
      "Epoch 61/1000 - Loss: 2.5327211966299576\n",
      "Epoch 62/1000 - Loss: 2.5241427604581044\n",
      "Epoch 63/1000 - Loss: 2.5155426465279134\n",
      "Epoch 64/1000 - Loss: 2.506919789636374\n",
      "Epoch 65/1000 - Loss: 2.498273178144558\n",
      "Epoch 66/1000 - Loss: 2.489601853979016\n",
      "Epoch 67/1000 - Loss: 2.480904912642761\n",
      "Epoch 68/1000 - Loss: 2.4721815032328136\n",
      "Epoch 69/1000 - Loss: 2.4634308284612616\n",
      "Epoch 70/1000 - Loss: 2.454652144676766\n",
      "Epoch 71/1000 - Loss: 2.445844761883394\n",
      "Epoch 72/1000 - Loss: 2.4370080437536537\n",
      "Epoch 73/1000 - Loss: 2.428141407632574\n",
      "Epoch 74/1000 - Loss: 2.4192443245296604\n",
      "Epoch 75/1000 - Loss: 2.4103163190955468\n",
      "Epoch 76/1000 - Loss: 2.4013569695801578\n",
      "Epoch 77/1000 - Loss: 2.3923659077692054\n",
      "Epoch 78/1000 - Loss: 2.383342818895847\n",
      "Epoch 79/1000 - Loss: 2.3742874415243507\n",
      "Epoch 80/1000 - Loss: 2.3651995674026693\n",
      "Epoch 81/1000 - Loss: 2.356079041280812\n",
      "Epoch 82/1000 - Loss: 2.346925760692008\n",
      "Epoch 83/1000 - Loss: 2.337739675693688\n",
      "Epoch 84/1000 - Loss: 2.3285207885653785\n",
      "Epoch 85/1000 - Loss: 2.319269153460717\n",
      "Epoch 86/1000 - Loss: 2.309984876010867\n",
      "Epoch 87/1000 - Loss: 2.3006681128767403\n",
      "Epoch 88/1000 - Loss: 2.2913190712475604\n",
      "Epoch 89/1000 - Loss: 2.281938008283434\n",
      "Epoch 90/1000 - Loss: 2.272525230499759\n",
      "Epoch 91/1000 - Loss: 2.2630810930914746\n",
      "Epoch 92/1000 - Loss: 2.253605999195316\n",
      "Epoch 93/1000 - Loss: 2.244100399088483\n",
      "Epoch 94/1000 - Loss: 2.2345647893222864\n",
      "Epoch 95/1000 - Loss: 2.2249997117896223\n",
      "Epoch 96/1000 - Loss: 2.2154057527253195\n",
      "Epoch 97/1000 - Loss: 2.2057835416386826\n",
      "Epoch 98/1000 - Loss: 2.196133750177819\n",
      "Epoch 99/1000 - Loss: 2.1864570909256074\n",
      "Epoch 100/1000 - Loss: 2.1767543161274494\n",
      "Epoch 101/1000 - Loss: 2.167026216351272\n",
      "Epoch 102/1000 - Loss: 2.1572736190805077\n",
      "Epoch 103/1000 - Loss: 2.147497387241141\n",
      "Epoch 104/1000 - Loss: 2.137698417664177\n",
      "Epoch 105/1000 - Loss: 2.1278776394852605\n",
      "Epoch 106/1000 - Loss: 2.118036012483449\n",
      "Epoch 107/1000 - Loss: 2.1081745253614965\n",
      "Epoch 108/1000 - Loss: 2.0982941939703137\n",
      "Epoch 109/1000 - Loss: 2.0883960594805826\n",
      "Epoch 110/1000 - Loss: 2.0784811865048236\n",
      "Epoch 111/1000 - Loss: 2.0685506611735045\n",
      "Epoch 112/1000 - Loss: 2.0586055891690807\n",
      "Epoch 113/1000 - Loss: 2.048647093722128\n",
      "Epoch 114/1000 - Loss: 2.038676313573997\n",
      "Epoch 115/1000 - Loss: 2.028694400910665\n",
      "Epoch 116/1000 - Loss: 2.018702519272691\n",
      "Epoch 117/1000 - Loss: 2.0087018414463884\n",
      "Epoch 118/1000 - Loss: 1.9986935473415073\n",
      "Epoch 119/1000 - Loss: 1.9886788218608962\n",
      "Epoch 120/1000 - Loss: 1.9786588527677331\n",
      "Epoch 121/1000 - Loss: 1.9686348285560384\n",
      "Epoch 122/1000 - Loss: 1.9586079363302502\n",
      "Epoch 123/1000 - Loss: 1.9485793596997039\n",
      "Epoch 124/1000 - Loss: 1.9385502766938836\n",
      "Epoch 125/1000 - Loss: 1.9285218577042846\n",
      "Epoch 126/1000 - Loss: 1.9184952634587167\n",
      "Epoch 127/1000 - Loss: 1.9084716430337785\n",
      "Epoch 128/1000 - Loss: 1.898452131911157\n",
      "Epoch 129/1000 - Loss: 1.8884378500832595\n",
      "Epoch 130/1000 - Loss: 1.8784299002135358\n",
      "Epoch 131/1000 - Loss: 1.8684293658566533\n",
      "Epoch 132/1000 - Loss: 1.8584373097434748\n",
      "Epoch 133/1000 - Loss: 1.8484547721355549\n",
      "Epoch 134/1000 - Loss: 1.8384827692535897\n",
      "Epoch 135/1000 - Loss: 1.8285222917839827\n",
      "Epoch 136/1000 - Loss: 1.818574303467374\n",
      "Epoch 137/1000 - Loss: 1.8086397397726524\n",
      "Epoch 138/1000 - Loss: 1.7987195066596358\n",
      "Epoch 139/1000 - Loss: 1.7888144794332432\n",
      "Epoch 140/1000 - Loss: 1.7789255016916157\n",
      "Epoch 141/1000 - Loss: 1.7690533843702716\n",
      "Epoch 142/1000 - Loss: 1.7591989048840007\n",
      "Epoch 143/1000 - Loss: 1.749362806367818\n",
      "Epoch 144/1000 - Loss: 1.739545797017908\n",
      "Epoch 145/1000 - Loss: 1.7297485495331173\n",
      "Epoch 146/1000 - Loss: 1.7199717006571666\n",
      "Epoch 147/1000 - Loss: 1.710215850821387\n",
      "Epoch 148/1000 - Loss: 1.7004815638874142\n",
      "Epoch 149/1000 - Loss: 1.690769366988936\n",
      "Epoch 150/1000 - Loss: 1.681079750471235\n",
      "Epoch 151/1000 - Loss: 1.671413167926949\n",
      "Epoch 152/1000 - Loss: 1.6617700363261734\n",
      "Epoch 153/1000 - Loss: 1.6521507362387153\n",
      "Epoch 154/1000 - Loss: 1.6425556121460634\n",
      "Epoch 155/1000 - Loss: 1.6329849728403705\n",
      "Epoch 156/1000 - Loss: 1.6234390919075155\n",
      "Epoch 157/1000 - Loss: 1.613918208291106\n",
      "Epoch 158/1000 - Loss: 1.6044225269340966\n",
      "Epoch 159/1000 - Loss: 1.594952219494512\n",
      "Epoch 160/1000 - Loss: 1.5855074251316577\n",
      "Epoch 161/1000 - Loss: 1.5760882513590384\n",
      "Epoch 162/1000 - Loss: 1.5666947749601234\n",
      "Epoch 163/1000 - Loss: 1.5573270429630204\n",
      "Epoch 164/1000 - Loss: 1.547985073670049\n",
      "Epoch 165/1000 - Loss: 1.538668857738172\n",
      "Epoch 166/1000 - Loss: 1.5293783593062198\n",
      "Epoch 167/1000 - Loss: 1.5201135171648361\n",
      "Epoch 168/1000 - Loss: 1.5108742459650948\n",
      "Epoch 169/1000 - Loss: 1.5016604374617566\n",
      "Epoch 170/1000 - Loss: 1.492471961787185\n",
      "Epoch 171/1000 - Loss: 1.483308668751999\n",
      "Epoch 172/1000 - Loss: 1.4741703891685973\n",
      "Epoch 173/1000 - Loss: 1.4650569361937857\n",
      "Epoch 174/1000 - Loss: 1.4559681066868166\n",
      "Epoch 175/1000 - Loss: 1.44690368257925\n",
      "Epoch 176/1000 - Loss: 1.4378634322531512\n",
      "Epoch 177/1000 - Loss: 1.4288471119242583\n",
      "Epoch 178/1000 - Loss: 1.4198544670268527\n",
      "Epoch 179/1000 - Loss: 1.4108852335972\n",
      "Epoch 180/1000 - Loss: 1.401939139652552\n",
      "Epoch 181/1000 - Loss: 1.3930159065628063\n",
      "Epoch 182/1000 - Loss: 1.3841152504120724\n",
      "Epoch 183/1000 - Loss: 1.3752368833475004\n",
      "Epoch 184/1000 - Loss: 1.3663805149128545\n",
      "Epoch 185/1000 - Loss: 1.3575458533644462\n",
      "Epoch 186/1000 - Loss: 1.3487326069671601\n",
      "Epoch 187/1000 - Loss: 1.3399404852684187\n",
      "Epoch 188/1000 - Loss: 1.3311692003480555\n",
      "Epoch 189/1000 - Loss: 1.3224184680421835\n",
      "Epoch 190/1000 - Loss: 1.3136880091392398\n",
      "Epoch 191/1000 - Loss: 1.3049775505465129\n",
      "Epoch 192/1000 - Loss: 1.2962868264255438\n",
      "Epoch 193/1000 - Loss: 1.287615579294906\n",
      "Epoch 194/1000 - Loss: 1.2789635610989454\n",
      "Epoch 195/1000 - Loss: 1.2703305342411704\n",
      "Epoch 196/1000 - Loss: 1.2617162725810445\n",
      "Epoch 197/1000 - Loss: 1.2531205623930477\n",
      "Epoch 198/1000 - Loss: 1.2445432032869088\n",
      "Epoch 199/1000 - Loss: 1.2359840090880263\n",
      "Epoch 200/1000 - Loss: 1.2274428086771367\n",
      "Epoch 201/1000 - Loss: 1.2189194467883686\n",
      "Epoch 202/1000 - Loss: 1.2104137847648833\n",
      "Epoch 203/1000 - Loss: 1.20192570127137\n",
      "Epoch 204/1000 - Loss: 1.1934550929627041\n",
      "Epoch 205/1000 - Loss: 1.1850018751081637\n",
      "Epoch 206/1000 - Loss: 1.1765659821706285\n",
      "Epoch 207/1000 - Loss: 1.1681473683402503\n",
      "Epoch 208/1000 - Loss: 1.1597460080221476\n",
      "Epoch 209/1000 - Loss: 1.151361896277706\n",
      "Epoch 210/1000 - Loss: 1.1429950492191419\n",
      "Epoch 211/1000 - Loss: 1.1346455043570263\n",
      "Epoch 212/1000 - Loss: 1.1263133209005212\n",
      "Epoch 213/1000 - Loss: 1.1179985800101344\n",
      "Epoch 214/1000 - Loss: 1.1097013850028548\n",
      "Epoch 215/1000 - Loss: 1.1014218615095825\n",
      "Epoch 216/1000 - Loss: 1.0931601575848273\n",
      "Epoch 217/1000 - Loss: 1.0849164437687047\n",
      "Epoch 218/1000 - Loss: 1.0766909131013231\n",
      "Epoch 219/1000 - Loss: 1.0684837810897168\n",
      "Epoch 220/1000 - Loss: 1.060295285627534\n",
      "Epoch 221/1000 - Loss: 1.0521256868677789\n",
      "Epoch 222/1000 - Loss: 1.0439752670489424\n",
      "Epoch 223/1000 - Loss: 1.0358443302749616\n",
      "Epoch 224/1000 - Loss: 1.027733202249492\n",
      "Epoch 225/1000 - Loss: 1.0196422299650725\n",
      "Epoch 226/1000 - Loss: 1.011571781347827\n",
      "Epoch 227/1000 - Loss: 1.0035222448584333\n",
      "Epoch 228/1000 - Loss: 0.9954940290501654\n",
      "Epoch 229/1000 - Loss: 0.9874875620849051\n",
      "Epoch 230/1000 - Loss: 0.9795032912080965\n",
      "Epoch 231/1000 - Loss: 0.9715416821836991\n",
      "Epoch 232/1000 - Loss: 0.963603218690293\n",
      "Epoch 233/1000 - Loss: 0.9556884016795598\n",
      "Epoch 234/1000 - Loss: 0.9477977486984682\n",
      "Epoch 235/1000 - Loss: 0.9399317931765528\n",
      "Epoch 236/1000 - Loss: 0.9320910836797867\n",
      "Epoch 237/1000 - Loss: 0.9242761831326163\n",
      "Epoch 238/1000 - Loss: 0.9164876680098076\n",
      "Epoch 239/1000 - Loss: 0.908726127499845\n",
      "Epoch 240/1000 - Loss: 0.9009921626416895\n",
      "Epoch 241/1000 - Loss: 0.8932863854367805\n",
      "Epoch 242/1000 - Loss: 0.8856094179382417\n",
      "Epoch 243/1000 - Loss: 0.8779618913193029\n",
      "Epoch 244/1000 - Loss: 0.870344444923036\n",
      "Epoch 245/1000 - Loss: 0.862757725295533\n",
      "Epoch 246/1000 - Loss: 0.8552023852047217\n",
      "Epoch 247/1000 - Loss: 0.8476790826470606\n",
      "Epoch 248/1000 - Loss: 0.8401884798443828\n",
      "Epoch 249/1000 - Loss: 0.8327312422332\n",
      "Epoch 250/1000 - Loss: 0.8253080374488002\n",
      "Epoch 251/1000 - Loss: 0.817919534306492\n",
      "Epoch 252/1000 - Loss: 0.8105664017823537\n",
      "Epoch 253/1000 - Loss: 0.8032493079958516\n",
      "Epoch 254/1000 - Loss: 0.7959689191966921\n",
      "Epoch 255/1000 - Loss: 0.7887258987582397\n",
      "Epoch 256/1000 - Loss: 0.7815209061798354\n",
      "Epoch 257/1000 - Loss: 0.7743545961002998\n",
      "Epoch 258/1000 - Loss: 0.76722761732487\n",
      "Epoch 259/1000 - Loss: 0.7601406118677899\n",
      "Epoch 260/1000 - Loss: 0.7530942140126878\n",
      "Epoch 261/1000 - Loss: 0.7460890493928373\n",
      "Epoch 262/1000 - Loss: 0.7391257340933179\n",
      "Epoch 263/1000 - Loss: 0.7322048737770164\n",
      "Epoch 264/1000 - Loss: 0.7253270628363094\n",
      "Epoch 265/1000 - Loss: 0.7184928835722107\n",
      "Epoch 266/1000 - Loss: 0.7117029054026253\n",
      "Epoch 267/1000 - Loss: 0.7049576841012912\n",
      "Epoch 268/1000 - Loss: 0.6982577610688405\n",
      "Epoch 269/1000 - Loss: 0.6916036626373344\n",
      "Epoch 270/1000 - Loss: 0.6849958994094849\n",
      "Epoch 271/1000 - Loss: 0.6784349656336655\n",
      "Epoch 272/1000 - Loss: 0.6719213386156818\n",
      "Epoch 273/1000 - Loss: 0.6654554781681619\n",
      "Epoch 274/1000 - Loss: 0.6590378260982772\n",
      "Epoch 275/1000 - Loss: 0.652668805734393\n",
      "Epoch 276/1000 - Loss: 0.6463488214921126\n",
      "Epoch 277/1000 - Loss: 0.6400782584800498\n",
      "Epoch 278/1000 - Loss: 0.633857482145537\n",
      "Epoch 279/1000 - Loss: 0.6276868379603533\n",
      "Epoch 280/1000 - Loss: 0.6215666511464232\n",
      "Epoch 281/1000 - Loss: 0.6154972264413361\n",
      "Epoch 282/1000 - Loss: 0.60947884790339\n",
      "Epoch 283/1000 - Loss: 0.6035117787557742\n",
      "Epoch 284/1000 - Loss: 0.5975962612693826\n",
      "Epoch 285/1000 - Loss: 0.5917325166836478\n",
      "Epoch 286/1000 - Loss: 0.5859207451646816\n",
      "Epoch 287/1000 - Loss: 0.5801611257999264\n",
      "Epoch 288/1000 - Loss: 0.5744538166284124\n",
      "Epoch 289/1000 - Loss: 0.5687989547056499\n",
      "Epoch 290/1000 - Loss: 0.5631966562021062\n",
      "Epoch 291/1000 - Loss: 0.5576470165341344\n",
      "Epoch 292/1000 - Loss: 0.5521501105261778\n",
      "Epoch 293/1000 - Loss: 0.5467059926029987\n",
      "Epoch 294/1000 - Loss: 0.541314697010638\n",
      "Epoch 295/1000 - Loss: 0.5359762380647758\n",
      "Epoch 296/1000 - Loss: 0.5306906104251043\n",
      "Epoch 297/1000 - Loss: 0.5254577893943209\n",
      "Epoch 298/1000 - Loss: 0.5202777312403087\n",
      "Epoch 299/1000 - Loss: 0.5151503735400524\n",
      "Epoch 300/1000 - Loss: 0.5100756355438415\n",
      "Epoch 301/1000 - Loss: 0.5050534185582887\n",
      "Epoch 302/1000 - Loss: 0.5000836063467079\n",
      "Epoch 303/1000 - Loss: 0.49516606554538883\n",
      "Epoch 304/1000 - Loss: 0.49030064609432333\n",
      "Epoch 305/1000 - Loss: 0.4854871816809566\n",
      "Epoch 306/1000 - Loss: 0.4807254901955446\n",
      "Epoch 307/1000 - Loss: 0.4760153741967419\n",
      "Epoch 308/1000 - Loss: 0.47135662138605383\n",
      "Epoch 309/1000 - Loss: 0.4667490050898335\n",
      "Epoch 310/1000 - Loss: 0.46219228474752966\n",
      "Epoch 311/1000 - Loss: 0.45768620640494173\n",
      "Epoch 312/1000 - Loss: 0.45323050321125846\n",
      "Epoch 313/1000 - Loss: 0.4488248959187307\n",
      "Epoch 314/1000 - Loss: 0.4444690933838385\n",
      "Epoch 315/1000 - Loss: 0.44016279306889566\n",
      "Epoch 316/1000 - Loss: 0.43590568154304876\n",
      "Epoch 317/1000 - Loss: 0.4316974349817091\n",
      "Epoch 318/1000 - Loss: 0.42753771966348086\n",
      "Epoch 319/1000 - Loss: 0.42342619246371116\n",
      "Epoch 320/1000 - Loss: 0.4193625013438462\n",
      "Epoch 321/1000 - Loss: 0.4153462858358088\n",
      "Epoch 322/1000 - Loss: 0.4113771775206801\n",
      "Epoch 323/1000 - Loss: 0.4074548005010173\n",
      "Epoch 324/1000 - Loss: 0.4035787718661762\n",
      "Epoch 325/1000 - Loss: 0.3997487021500789\n",
      "Epoch 326/1000 - Loss: 0.3959641957808926\n",
      "Epoch 327/1000 - Loss: 0.392224851522144\n",
      "Epoch 328/1000 - Loss: 0.388530262904846\n",
      "Epoch 329/1000 - Loss: 0.3848800186502407\n",
      "Epoch 330/1000 - Loss: 0.3812737030828289\n",
      "Epoch 331/1000 - Loss: 0.37771089653337236\n",
      "Epoch 332/1000 - Loss: 0.3741911757316205\n",
      "Epoch 333/1000 - Loss: 0.37071411418853284\n",
      "Epoch 334/1000 - Loss: 0.36727928256782116\n",
      "Epoch 335/1000 - Loss: 0.3638862490466544\n",
      "Epoch 336/1000 - Loss: 0.3605345796654201\n",
      "Epoch 337/1000 - Loss: 0.3572238386664541\n",
      "Epoch 338/1000 - Loss: 0.3539535888216908\n",
      "Epoch 339/1000 - Loss: 0.3507233917492061\n",
      "Epoch 340/1000 - Loss: 0.34753280821866184\n",
      "Epoch 341/1000 - Loss: 0.3443813984456764\n",
      "Epoch 342/1000 - Loss: 0.34126872237517347\n",
      "Epoch 343/1000 - Loss: 0.3381943399537835\n",
      "Epoch 344/1000 - Loss: 0.33515781139139605\n",
      "Epoch 345/1000 - Loss: 0.33215869741196785\n",
      "Epoch 346/1000 - Loss: 0.3291965594937226\n",
      "Epoch 347/1000 - Loss: 0.32627096009888534\n",
      "Epoch 348/1000 - Loss: 0.3233814628931121\n",
      "Epoch 349/1000 - Loss: 0.32052763295478703\n",
      "Epoch 350/1000 - Loss: 0.3177090369743697\n",
      "Epoch 351/1000 - Loss: 0.3149252434439881\n",
      "Epoch 352/1000 - Loss: 0.3121758228374847\n",
      "Epoch 353/1000 - Loss: 0.309460347781114\n",
      "Epoch 354/1000 - Loss: 0.30677839321512695\n",
      "Epoch 355/1000 - Loss: 0.30412953654645103\n",
      "Epoch 356/1000 - Loss: 0.3015133577927033\n",
      "Epoch 357/1000 - Loss: 0.29892943971776353\n",
      "Epoch 358/1000 - Loss: 0.2963773679591493\n",
      "Epoch 359/1000 - Loss: 0.2938567311474203\n",
      "Epoch 360/1000 - Loss: 0.29136712101786244\n",
      "Epoch 361/1000 - Loss: 0.2889081325146816\n",
      "Epoch 362/1000 - Loss: 0.2864793638879532\n",
      "Epoch 363/1000 - Loss: 0.28408041678356355\n",
      "Epoch 364/1000 - Loss: 0.28171089632637947\n",
      "Epoch 365/1000 - Loss: 0.2793704111968899\n",
      "Epoch 366/1000 - Loss: 0.2770585737015435\n",
      "Epoch 367/1000 - Loss: 0.274774999837022\n",
      "Epoch 368/1000 - Loss: 0.2725193093486754\n",
      "Epoch 369/1000 - Loss: 0.27029112578334513\n",
      "Epoch 370/1000 - Loss: 0.2680900765367921\n",
      "Epoch 371/1000 - Loss: 0.26591579289596345\n",
      "Epoch 372/1000 - Loss: 0.263767910076289\n",
      "Epoch 373/1000 - Loss: 0.2616460672542377\n",
      "Epoch 374/1000 - Loss: 0.2595499075953294\n",
      "Epoch 375/1000 - Loss: 0.25747907827780575\n",
      "Epoch 376/1000 - Loss: 0.2554332305121523\n",
      "Epoch 377/1000 - Loss: 0.25341201955666987\n",
      "Epoch 378/1000 - Loss: 0.2514151047292763\n",
      "Epoch 379/1000 - Loss: 0.249442149415719\n",
      "Epoch 380/1000 - Loss: 0.24749282107437834\n",
      "Epoch 381/1000 - Loss: 0.24556679123783182\n",
      "Epoch 382/1000 - Loss: 0.24366373551133796\n",
      "Epoch 383/1000 - Loss: 0.2417833335684149\n",
      "Epoch 384/1000 - Loss: 0.2399252691436553\n",
      "Epoch 385/1000 - Loss: 0.2380892300229327\n",
      "Epoch 386/1000 - Loss: 0.2362749080311501\n",
      "Epoch 387/1000 - Loss: 0.23448199901766434\n",
      "Epoch 388/1000 - Loss: 0.23271020283952315\n",
      "Epoch 389/1000 - Loss: 0.23095922334264876\n",
      "Epoch 390/1000 - Loss: 0.2292287683410919\n",
      "Epoch 391/1000 - Loss: 0.22751854959447465\n",
      "Epoch 392/1000 - Loss: 0.22582828278374617\n",
      "Epoch 393/1000 - Loss: 0.22415768748535436\n",
      "Epoch 394/1000 - Loss: 0.2225064871439449\n",
      "Epoch 395/1000 - Loss: 0.2208744090436953\n",
      "Epoch 396/1000 - Loss: 0.21926118427837638\n",
      "Epoch 397/1000 - Loss: 0.21766654772023636\n",
      "Epoch 398/1000 - Loss: 0.21609023798780275\n",
      "Epoch 399/1000 - Loss: 0.2145319974126888\n",
      "Epoch 400/1000 - Loss: 0.21299157200547972\n",
      "Epoch 401/1000 - Loss: 0.21146871142078805\n",
      "Epoch 402/1000 - Loss: 0.20996316892154812\n",
      "Epoch 403/1000 - Loss: 0.2084747013426232\n",
      "Epoch 404/1000 - Loss: 0.20700306905379298\n",
      "Epoch 405/1000 - Loss: 0.20554803592219123\n",
      "Epoch 406/1000 - Loss: 0.20410936927425177\n",
      "Epoch 407/1000 - Loss: 0.2026868398572189\n",
      "Epoch 408/1000 - Loss: 0.2012802218002926\n",
      "Epoch 409/1000 - Loss: 0.19988929257544322\n",
      "Epoch 410/1000 - Loss: 0.19851383295796304\n",
      "Epoch 411/1000 - Loss: 0.19715362698679004\n",
      "Epoch 412/1000 - Loss: 0.19580846192465887\n",
      "Epoch 413/1000 - Loss: 0.19447812821811847\n",
      "Epoch 414/1000 - Loss: 0.1931624194574539\n",
      "Epoch 415/1000 - Loss: 0.19186113233655835\n",
      "Epoch 416/1000 - Loss: 0.19057406661278353\n",
      "Epoch 417/1000 - Loss: 0.1893010250668098\n",
      "Epoch 418/1000 - Loss: 0.18804181346256174\n",
      "Epoch 419/1000 - Loss: 0.18679624050720753\n",
      "Epoch 420/1000 - Loss: 0.18556411781125987\n",
      "Epoch 421/1000 - Loss: 0.18434525984881622\n",
      "Epoch 422/1000 - Loss: 0.18313948391795432\n",
      "Epoch 423/1000 - Loss: 0.1819466101013133\n",
      "Epoch 424/1000 - Loss: 0.18076646122687862\n",
      "Epoch 425/1000 - Loss: 0.17959886282898826\n",
      "Epoch 426/1000 - Loss: 0.17844364310958905\n",
      "Epoch 427/1000 - Loss: 0.17730063289974843\n",
      "Epoch 428/1000 - Loss: 0.17616966562144604\n",
      "Epoch 429/1000 - Loss: 0.17505057724965895\n",
      "Epoch 430/1000 - Loss: 0.17394320627475301\n",
      "Epoch 431/1000 - Loss: 0.17284739366519092\n",
      "Epoch 432/1000 - Loss: 0.17176298283057775\n",
      "Epoch 433/1000 - Loss: 0.17068981958504092\n",
      "Epoch 434/1000 - Loss: 0.1696277521109673\n",
      "Epoch 435/1000 - Loss: 0.16857663092309882\n",
      "Epoch 436/1000 - Loss: 0.16753630883299783\n",
      "Epoch 437/1000 - Loss: 0.16650664091388598\n",
      "Epoch 438/1000 - Loss: 0.16548748446587025\n",
      "Epoch 439/1000 - Loss: 0.16447869898154935\n",
      "Epoch 440/1000 - Loss: 0.16348014611202288\n",
      "Epoch 441/1000 - Loss: 0.1624916896332888\n",
      "Epoch 442/1000 - Loss: 0.16151319541304907\n",
      "Epoch 443/1000 - Loss: 0.160544531377913\n",
      "Epoch 444/1000 - Loss: 0.15958556748101388\n",
      "Epoch 445/1000 - Loss: 0.1586361756700317\n",
      "Epoch 446/1000 - Loss: 0.15769622985562456\n",
      "Epoch 447/1000 - Loss: 0.15676560588027563\n",
      "Epoch 448/1000 - Loss: 0.1558441814875486\n",
      "Epoch 449/1000 - Loss: 0.15493183629175825\n",
      "Epoch 450/1000 - Loss: 0.1540284517480547\n",
      "Epoch 451/1000 - Loss: 0.15313391112291408\n",
      "Epoch 452/1000 - Loss: 0.1522480994650483\n",
      "Epoch 453/1000 - Loss: 0.15137090357671626\n",
      "Epoch 454/1000 - Loss: 0.15050221198545274\n",
      "Epoch 455/1000 - Loss: 0.14964191491619674\n",
      "Epoch 456/1000 - Loss: 0.14878990426382868\n",
      "Epoch 457/1000 - Loss: 0.1479460735661097\n",
      "Epoch 458/1000 - Loss: 0.14711031797702043\n",
      "Epoch 459/1000 - Loss: 0.14628253424050058\n",
      "Epoch 460/1000 - Loss: 0.14546262066457732\n",
      "Epoch 461/1000 - Loss: 0.1446504770958931\n",
      "Epoch 462/1000 - Loss: 0.1438460048946174\n",
      "Epoch 463/1000 - Loss: 0.14304910690974537\n",
      "Epoch 464/1000 - Loss: 0.14225968745477774\n",
      "Epoch 465/1000 - Loss: 0.14147765228378165\n",
      "Epoch 466/1000 - Loss: 0.14070290856782477\n",
      "Epoch 467/1000 - Loss: 0.1399353648717768\n",
      "Epoch 468/1000 - Loss: 0.13917493113148774\n",
      "Epoch 469/1000 - Loss: 0.13842151863131968\n",
      "Epoch 470/1000 - Loss: 0.13767503998204295\n",
      "Epoch 471/1000 - Loss: 0.1369354090990861\n",
      "Epoch 472/1000 - Loss: 0.13620254118113642\n",
      "Epoch 473/1000 - Loss: 0.1354763526890897\n",
      "Epoch 474/1000 - Loss: 0.13475676132533926\n",
      "Epoch 475/1000 - Loss: 0.13404368601340538\n",
      "Epoch 476/1000 - Loss: 0.13333704687789757\n",
      "Epoch 477/1000 - Loss: 0.13263676522480833\n",
      "Epoch 478/1000 - Loss: 0.13194276352213072\n",
      "Epoch 479/1000 - Loss: 0.13125496538079567\n",
      "Epoch 480/1000 - Loss: 0.13057329553593042\n",
      "Epoch 481/1000 - Loss: 0.1298976798284232\n",
      "Epoch 482/1000 - Loss: 0.12922804518680125\n",
      "Epoch 483/1000 - Loss: 0.12856431960940792\n",
      "Epoch 484/1000 - Loss: 0.12790643214688183\n",
      "Epoch 485/1000 - Loss: 0.12725431288493047\n",
      "Epoch 486/1000 - Loss: 0.12660789292739247\n",
      "Epoch 487/1000 - Loss: 0.12596710437958797\n",
      "Epoch 488/1000 - Loss: 0.12533188033194986\n",
      "Epoch 489/1000 - Loss: 0.1247021548439331\n",
      "Epoch 490/1000 - Loss: 0.12407786292819604\n",
      "Epoch 491/1000 - Loss: 0.12345894053505421\n",
      "Epoch 492/1000 - Loss: 0.12284532453719438\n",
      "Epoch 493/1000 - Loss: 0.12223695271465371\n",
      "Epoch 494/1000 - Loss: 0.12163376374005311\n",
      "Epoch 495/1000 - Loss: 0.12103569716408386\n",
      "Epoch 496/1000 - Loss: 0.12044269340124385\n",
      "Epoch 497/1000 - Loss: 0.11985469371581513\n",
      "Epoch 498/1000 - Loss: 0.11927164020808781\n",
      "Epoch 499/1000 - Loss: 0.11869347580081475\n",
      "Epoch 500/1000 - Loss: 0.11812014422590253\n",
      "Epoch 501/1000 - Loss: 0.11755159001132914\n",
      "Epoch 502/1000 - Loss: 0.11698775846829235\n",
      "Epoch 503/1000 - Loss: 0.11642859567856993\n",
      "Epoch 504/1000 - Loss: 0.11587404848210707\n",
      "Epoch 505/1000 - Loss: 0.11532406446481386\n",
      "Epoch 506/1000 - Loss: 0.11477859194657168\n",
      "Epoch 507/1000 - Loss: 0.11423757996944817\n",
      "Epoch 508/1000 - Loss: 0.11370097828611866\n",
      "Epoch 509/1000 - Loss: 0.11316873734848287\n",
      "Epoch 510/1000 - Loss: 0.1126408082964778\n",
      "Epoch 511/1000 - Loss: 0.11211714294709103\n",
      "Epoch 512/1000 - Loss: 0.11159769378355444\n",
      "Epoch 513/1000 - Loss: 0.11108241394473084\n",
      "Epoch 514/1000 - Loss: 0.11057125721468135\n",
      "Epoch 515/1000 - Loss: 0.11006417801241514\n",
      "Epoch 516/1000 - Loss: 0.1095611313818118\n",
      "Epoch 517/1000 - Loss: 0.10906207298172363\n",
      "Epoch 518/1000 - Loss: 0.10856695907624321\n",
      "Epoch 519/1000 - Loss: 0.1080757465251429\n",
      "Epoch 520/1000 - Loss: 0.1075883927744769\n",
      "Epoch 521/1000 - Loss: 0.10710485584734736\n",
      "Epoch 522/1000 - Loss: 0.10662509433482656\n",
      "Epoch 523/1000 - Loss: 0.10614906738703946\n",
      "Epoch 524/1000 - Loss: 0.10567673470439695\n",
      "Epoch 525/1000 - Loss: 0.10520805652898024\n",
      "Epoch 526/1000 - Loss: 0.10474299363607582\n",
      "Epoch 527/1000 - Loss: 0.10428150732585195\n",
      "Epoch 528/1000 - Loss: 0.10382355941518252\n",
      "Epoch 529/1000 - Loss: 0.10336911222960882\n",
      "Epoch 530/1000 - Loss: 0.10291812859543875\n",
      "Epoch 531/1000 - Loss: 0.10247057183198346\n",
      "Epoch 532/1000 - Loss: 0.10202640574392483\n",
      "Epoch 533/1000 - Loss: 0.10158559461381789\n",
      "Epoch 534/1000 - Loss: 0.10114810319471343\n",
      "Epoch 535/1000 - Loss: 0.10071389670291689\n",
      "Epoch 536/1000 - Loss: 0.10028294081086195\n",
      "Epoch 537/1000 - Loss: 0.09985520164011309\n",
      "Epoch 538/1000 - Loss: 0.09943064575447944\n",
      "Epoch 539/1000 - Loss: 0.09900924015325539\n",
      "Epoch 540/1000 - Loss: 0.09859095226456738\n",
      "Epoch 541/1000 - Loss: 0.09817574993884018\n",
      "Epoch 542/1000 - Loss: 0.09776360144236976\n",
      "Epoch 543/1000 - Loss: 0.09735447545100924\n",
      "Epoch 544/1000 - Loss: 0.09694834104395764\n",
      "Epoch 545/1000 - Loss: 0.09654516769765667\n",
      "Epoch 546/1000 - Loss: 0.0961449252797904\n",
      "Epoch 547/1000 - Loss: 0.09574758404338621\n",
      "Epoch 548/1000 - Loss: 0.09535311462101181\n",
      "Epoch 549/1000 - Loss: 0.09496148801907767\n",
      "Epoch 550/1000 - Loss: 0.0945726756122282\n",
      "Epoch 551/1000 - Loss: 0.09418664913782948\n",
      "Epoch 552/1000 - Loss: 0.0938033806905514\n",
      "Epoch 553/1000 - Loss: 0.09342284271703857\n",
      "Epoch 554/1000 - Loss: 0.09304500801067156\n",
      "Epoch 555/1000 - Loss: 0.09266984970641529\n",
      "Epoch 556/1000 - Loss: 0.09229734127575205\n",
      "Epoch 557/1000 - Loss: 0.09192745652170542\n",
      "Epoch 558/1000 - Loss: 0.09156016957393559\n",
      "Epoch 559/1000 - Loss: 0.09119545488392898\n",
      "Epoch 560/1000 - Loss: 0.0908332872202589\n",
      "Epoch 561/1000 - Loss: 0.09047364166392913\n",
      "Epoch 562/1000 - Loss: 0.09011649360379262\n",
      "Epoch 563/1000 - Loss: 0.08976181873204793\n",
      "Epoch 564/1000 - Loss: 0.0894095930398082\n",
      "Epoch 565/1000 - Loss: 0.08905979281274504\n",
      "Epoch 566/1000 - Loss: 0.08871239462680311\n",
      "Epoch 567/1000 - Loss: 0.0883673753439849\n",
      "Epoch 568/1000 - Loss: 0.08802471210820664\n",
      "Epoch 569/1000 - Loss: 0.08768438234121989\n",
      "Epoch 570/1000 - Loss: 0.08734636373860183\n",
      "Epoch 571/1000 - Loss: 0.08701063426580935\n",
      "Epoch 572/1000 - Loss: 0.0866771721543012\n",
      "Epoch 573/1000 - Loss: 0.08634595589771753\n",
      "Epoch 574/1000 - Loss: 0.08601696424812828\n",
      "Epoch 575/1000 - Loss: 0.08569017621233699\n",
      "Epoch 576/1000 - Loss: 0.08536557104824925\n",
      "Epoch 577/1000 - Loss: 0.08504312826129565\n",
      "Epoch 578/1000 - Loss: 0.08472282760091543\n",
      "Epoch 579/1000 - Loss: 0.08440464905709905\n",
      "Epoch 580/1000 - Loss: 0.08408857285697925\n",
      "Epoch 581/1000 - Loss: 0.08377457946148605\n",
      "Epoch 582/1000 - Loss: 0.08346264956204869\n",
      "Epoch 583/1000 - Loss: 0.08315276407735486\n",
      "Epoch 584/1000 - Loss: 0.0828449041501579\n",
      "Epoch 585/1000 - Loss: 0.08253905114413916\n",
      "Epoch 586/1000 - Loss: 0.08223518664081972\n",
      "Epoch 587/1000 - Loss: 0.08193329243651727\n",
      "Epoch 588/1000 - Loss: 0.08163335053935813\n",
      "Epoch 589/1000 - Loss: 0.08133534316633118\n",
      "Epoch 590/1000 - Loss: 0.08103925274039227\n",
      "Epoch 591/1000 - Loss: 0.08074506188760952\n",
      "Epoch 592/1000 - Loss: 0.08045275343436455\n",
      "Epoch 593/1000 - Loss: 0.0801623104045821\n",
      "Epoch 594/1000 - Loss: 0.07987371601701883\n",
      "Epoch 595/1000 - Loss: 0.07958695368258395\n",
      "Epoch 596/1000 - Loss: 0.0793020070017087\n",
      "Epoch 597/1000 - Loss: 0.07901885976175212\n",
      "Epoch 598/1000 - Loss: 0.07873749593445198\n",
      "Epoch 599/1000 - Loss: 0.07845789967341217\n",
      "Epoch 600/1000 - Loss: 0.07818005531162997\n",
      "Epoch 601/1000 - Loss: 0.07790394735906546\n",
      "Epoch 602/1000 - Loss: 0.07762956050024292\n",
      "Epoch 603/1000 - Loss: 0.07735687959189326\n",
      "Epoch 604/1000 - Loss: 0.0770858896606324\n",
      "Epoch 605/1000 - Loss: 0.07681657590067571\n",
      "Epoch 606/1000 - Loss: 0.0765489236715877\n",
      "Epoch 607/1000 - Loss: 0.07628291849606449\n",
      "Epoch 608/1000 - Loss: 0.07601854605775393\n",
      "Epoch 609/1000 - Loss: 0.07575579219910698\n",
      "Epoch 610/1000 - Loss: 0.07549464291926236\n",
      "Epoch 611/1000 - Loss: 0.07523508437196483\n",
      "Epoch 612/1000 - Loss: 0.07497710286351213\n",
      "Epoch 613/1000 - Loss: 0.07472068485073742\n",
      "Epoch 614/1000 - Loss: 0.07446581693902105\n",
      "Epoch 615/1000 - Loss: 0.07421248588032944\n",
      "Epoch 616/1000 - Loss: 0.07396067857128888\n",
      "Epoch 617/1000 - Loss: 0.07371038205128469\n",
      "Epoch 618/1000 - Loss: 0.07346158350059256\n",
      "Epoch 619/1000 - Loss: 0.07321427023853137\n",
      "Epoch 620/1000 - Loss: 0.07296842972165414\n",
      "Epoch 621/1000 - Loss: 0.0727240495419581\n",
      "Epoch 622/1000 - Loss: 0.07248111742512386\n",
      "Epoch 623/1000 - Loss: 0.07223962122878183\n",
      "Epoch 624/1000 - Loss: 0.07199954894080385\n",
      "Epoch 625/1000 - Loss: 0.07176088867762281\n",
      "Epoch 626/1000 - Loss: 0.07152362868257017\n",
      "Epoch 627/1000 - Loss: 0.07128775732425009\n",
      "Epoch 628/1000 - Loss: 0.07105326309492443\n",
      "Epoch 629/1000 - Loss: 0.07082013460893306\n",
      "Epoch 630/1000 - Loss: 0.07058836060112939\n",
      "Epoch 631/1000 - Loss: 0.07035792992534429\n",
      "Epoch 632/1000 - Loss: 0.07012883155287034\n",
      "Epoch 633/1000 - Loss: 0.06990105457096844\n",
      "Epoch 634/1000 - Loss: 0.0696745881813973\n",
      "Epoch 635/1000 - Loss: 0.06944942169896326\n",
      "Epoch 636/1000 - Loss: 0.06922554455009303\n",
      "Epoch 637/1000 - Loss: 0.06900294627142606\n",
      "Epoch 638/1000 - Loss: 0.068781616508427\n",
      "Epoch 639/1000 - Loss: 0.06856154501402105\n",
      "Epoch 640/1000 - Loss: 0.06834272164724539\n",
      "Epoch 641/1000 - Loss: 0.06812513637192294\n",
      "Epoch 642/1000 - Loss: 0.06790877925535402\n",
      "Epoch 643/1000 - Loss: 0.06769364046702936\n",
      "Epoch 644/1000 - Loss: 0.06747971027735708\n",
      "Epoch 645/1000 - Loss: 0.06726697905641185\n",
      "Epoch 646/1000 - Loss: 0.06705543727270241\n",
      "Epoch 647/1000 - Loss: 0.06684507549195227\n",
      "Epoch 648/1000 - Loss: 0.06663588437590358\n",
      "Epoch 649/1000 - Loss: 0.06642785468113399\n",
      "Epoch 650/1000 - Loss: 0.06622097725789262\n",
      "Epoch 651/1000 - Loss: 0.06601524304895283\n",
      "Epoch 652/1000 - Loss: 0.06581064308847823\n",
      "Epoch 653/1000 - Loss: 0.06560716850090734\n",
      "Epoch 654/1000 - Loss: 0.06540481049985669\n",
      "Epoch 655/1000 - Loss: 0.0652035603870316\n",
      "Epoch 656/1000 - Loss: 0.06500340955116228\n",
      "Epoch 657/1000 - Loss: 0.06480434946694667\n",
      "Epoch 658/1000 - Loss: 0.06460637169401112\n",
      "Epoch 659/1000 - Loss: 0.06440946787588875\n",
      "Epoch 660/1000 - Loss: 0.06421362973900593\n",
      "Epoch 661/1000 - Loss: 0.06401884909168962\n",
      "Epoch 662/1000 - Loss: 0.06382511782318251\n",
      "Epoch 663/1000 - Loss: 0.06363242790267709\n",
      "Epoch 664/1000 - Loss: 0.0634407713783583\n",
      "Epoch 665/1000 - Loss: 0.06325014037646469\n",
      "Epoch 666/1000 - Loss: 0.06306052710035782\n",
      "Epoch 667/1000 - Loss: 0.06287192382960669\n",
      "Epoch 668/1000 - Loss: 0.06268432291908761\n",
      "Epoch 669/1000 - Loss: 0.06249771679808938\n",
      "Epoch 670/1000 - Loss: 0.062312097969437735\n",
      "Epoch 671/1000 - Loss: 0.06212745900862947\n",
      "Epoch 672/1000 - Loss: 0.061943792562976406\n",
      "Epoch 673/1000 - Loss: 0.061761091350763395\n",
      "Epoch 674/1000 - Loss: 0.06157934816041897\n",
      "Epoch 675/1000 - Loss: 0.06139855584969227\n",
      "Epoch 676/1000 - Loss: 0.06121870734484869\n",
      "Epoch 677/1000 - Loss: 0.061039795639868846\n",
      "Epoch 678/1000 - Loss: 0.06086181379566283\n",
      "Epoch 679/1000 - Loss: 0.06068475493929493\n",
      "Epoch 680/1000 - Loss: 0.060508612263217645\n",
      "Epoch 681/1000 - Loss: 0.06033337902451622\n",
      "Epoch 682/1000 - Loss: 0.06015904854416355\n",
      "Epoch 683/1000 - Loss: 0.059985614206285426\n",
      "Epoch 684/1000 - Loss: 0.059813069457436654\n",
      "Epoch 685/1000 - Loss: 0.059641407805881334\n",
      "Epoch 686/1000 - Loss: 0.05947062282089312\n",
      "Epoch 687/1000 - Loss: 0.05930070813205359\n",
      "Epoch 688/1000 - Loss: 0.059131657428567796\n",
      "Epoch 689/1000 - Loss: 0.05896346445858576\n",
      "Epoch 690/1000 - Loss: 0.05879612302853308\n",
      "Epoch 691/1000 - Loss: 0.05862962700245093\n",
      "Epoch 692/1000 - Loss: 0.05846397030134491\n",
      "Epoch 693/1000 - Loss: 0.05829914690254206\n",
      "Epoch 694/1000 - Loss: 0.05813515083905636\n",
      "Epoch 695/1000 - Loss: 0.05797197619896349\n",
      "Epoch 696/1000 - Loss: 0.05780961712478219\n",
      "Epoch 697/1000 - Loss: 0.05764806781286559\n",
      "Epoch 698/1000 - Loss: 0.05748732251279742\n",
      "Epoch 699/1000 - Loss: 0.057327375526802456\n",
      "Epoch 700/1000 - Loss: 0.05716822120915572\n",
      "Epoch 701/1000 - Loss: 0.057009853965606816\n",
      "Epoch 702/1000 - Loss: 0.05685226825280819\n",
      "Epoch 703/1000 - Loss: 0.05669545857775223\n",
      "Epoch 704/1000 - Loss: 0.05653941949721431\n",
      "Epoch 705/1000 - Loss: 0.056384145617204846\n",
      "Epoch 706/1000 - Loss: 0.056229631592425874\n",
      "Epoch 707/1000 - Loss: 0.0560758721257371\n",
      "Epoch 708/1000 - Loss: 0.05592286196762818\n",
      "Epoch 709/1000 - Loss: 0.05577059591569683\n",
      "Epoch 710/1000 - Loss: 0.05561906881413367\n",
      "Epoch 711/1000 - Loss: 0.05546827555321569\n",
      "Epoch 712/1000 - Loss: 0.05531821106880357\n",
      "Epoch 713/1000 - Loss: 0.05516887034184691\n",
      "Epoch 714/1000 - Loss: 0.05502024839789564\n",
      "Epoch 715/1000 - Loss: 0.05487234030661708\n",
      "Epoch 716/1000 - Loss: 0.054725141181318895\n",
      "Epoch 717/1000 - Loss: 0.054578646178479795\n",
      "Epoch 718/1000 - Loss: 0.05443285049728424\n",
      "Epoch 719/1000 - Loss: 0.05428774937916367\n",
      "Epoch 720/1000 - Loss: 0.054143338107344674\n",
      "Epoch 721/1000 - Loss: 0.05399961200639933\n",
      "Epoch 722/1000 - Loss: 0.05385656644180672\n",
      "Epoch 723/1000 - Loss: 0.0537141968195146\n",
      "Epoch 724/1000 - Loss: 0.05357249858550933\n",
      "Epoch 725/1000 - Loss: 0.05343146722539216\n",
      "Epoch 726/1000 - Loss: 0.05329109826395528\n",
      "Epoch 727/1000 - Loss: 0.05315138726477231\n",
      "Epoch 728/1000 - Loss: 0.05301232982978402\n",
      "Epoch 729/1000 - Loss: 0.052873921598897915\n",
      "Epoch 730/1000 - Loss: 0.05273615824958463\n",
      "Epoch 731/1000 - Loss: 0.052599035496486306\n",
      "Epoch 732/1000 - Loss: 0.052462549091026295\n",
      "Epoch 733/1000 - Loss: 0.05232669482102323\n",
      "Epoch 734/1000 - Loss: 0.05219146851031185\n",
      "Epoch 735/1000 - Loss: 0.05205686601836655\n",
      "Epoch 736/1000 - Loss: 0.051922883239931286\n",
      "Epoch 737/1000 - Loss: 0.05178951610465362\n",
      "Epoch 738/1000 - Loss: 0.05165676057672024\n",
      "Epoch 739/1000 - Loss: 0.05152461265450246\n",
      "Epoch 740/1000 - Loss: 0.0513930683702014\n",
      "Epoch 741/1000 - Loss: 0.05126212378949871\n",
      "Epoch 742/1000 - Loss: 0.05113177501121382\n",
      "Epoch 743/1000 - Loss: 0.05100201816696115\n",
      "Epoch 744/1000 - Loss: 0.05087284942081498\n",
      "Epoch 745/1000 - Loss: 0.050744264968977104\n",
      "Epoch 746/1000 - Loss: 0.050616261039447684\n",
      "Epoch 747/1000 - Loss: 0.050488833891701224\n",
      "Epoch 748/1000 - Loss: 0.05036197981636648\n",
      "Epoch 749/1000 - Loss: 0.05023569513490962\n",
      "Epoch 750/1000 - Loss: 0.05010997619932084\n",
      "Epoch 751/1000 - Loss: 0.049984819391805047\n",
      "Epoch 752/1000 - Loss: 0.04986022112447708\n",
      "Epoch 753/1000 - Loss: 0.04973617783905967\n",
      "Epoch 754/1000 - Loss: 0.049612686006584486\n",
      "Epoch 755/1000 - Loss: 0.04948974212709838\n",
      "Epoch 756/1000 - Loss: 0.0493673427293707\n",
      "Epoch 757/1000 - Loss: 0.049245484370607565\n",
      "Epoch 758/1000 - Loss: 0.0491241636361653\n",
      "Epoch 759/1000 - Loss: 0.049003377139269846\n",
      "Epoch 760/1000 - Loss: 0.04888312152074046\n",
      "Epoch 761/1000 - Loss: 0.04876339344871254\n",
      "Epoch 762/1000 - Loss: 0.04864418961836925\n",
      "Epoch 763/1000 - Loss: 0.048525506751670985\n",
      "Epoch 764/1000 - Loss: 0.04840734159709133\n",
      "Epoch 765/1000 - Loss: 0.04828969092935412\n",
      "Epoch 766/1000 - Loss: 0.04817255154917709\n",
      "Epoch 767/1000 - Loss: 0.048055920283013115\n",
      "Epoch 768/1000 - Loss: 0.04793979398279834\n",
      "Epoch 769/1000 - Loss: 0.047824169525702113\n",
      "Epoch 770/1000 - Loss: 0.04770904381388018\n",
      "Epoch 771/1000 - Loss: 0.04759441377423085\n",
      "Epoch 772/1000 - Loss: 0.047480276358151316\n",
      "Epoch 773/1000 - Loss: 0.04736662854130126\n",
      "Epoch 774/1000 - Loss: 0.04725346732336662\n",
      "Epoch 775/1000 - Loss: 0.04714078972782465\n",
      "Epoch 776/1000 - Loss: 0.04702859280171458\n",
      "Epoch 777/1000 - Loss: 0.04691687361540976\n",
      "Epoch 778/1000 - Loss: 0.04680562926239315\n",
      "Epoch 779/1000 - Loss: 0.0466948568590303\n",
      "Epoch 780/1000 - Loss: 0.04658455354435562\n",
      "Epoch 781/1000 - Loss: 0.046474716479848424\n",
      "Epoch 782/1000 - Loss: 0.046365342849222196\n",
      "Epoch 783/1000 - Loss: 0.04625642985820752\n",
      "Epoch 784/1000 - Loss: 0.04614797473434652\n",
      "Epoch 785/1000 - Loss: 0.046039974726779696\n",
      "Epoch 786/1000 - Loss: 0.04593242710604406\n",
      "Epoch 787/1000 - Loss: 0.04582532916386642\n",
      "Epoch 788/1000 - Loss: 0.04571867821296631\n",
      "Epoch 789/1000 - Loss: 0.04561247158685157\n",
      "Epoch 790/1000 - Loss: 0.04550670663962693\n",
      "Epoch 791/1000 - Loss: 0.0454013807457959\n",
      "Epoch 792/1000 - Loss: 0.04529649130007013\n",
      "Epoch 793/1000 - Loss: 0.045192035717178655\n",
      "Epoch 794/1000 - Loss: 0.04508801143167952\n",
      "Epoch 795/1000 - Loss: 0.04498441589777395\n",
      "Epoch 796/1000 - Loss: 0.04488124658912298\n",
      "Epoch 797/1000 - Loss: 0.04477850099866402\n",
      "Epoch 798/1000 - Loss: 0.04467617663843367\n",
      "Epoch 799/1000 - Loss: 0.0445742710393849\n",
      "Epoch 800/1000 - Loss: 0.04447278175121682\n",
      "Epoch 801/1000 - Loss: 0.04437170634219593\n",
      "Epoch 802/1000 - Loss: 0.044271042398986646\n",
      "Epoch 803/1000 - Loss: 0.044170787526480466\n",
      "Epoch 804/1000 - Loss: 0.044070939347626216\n",
      "Epoch 805/1000 - Loss: 0.043971495503264915\n",
      "Epoch 806/1000 - Loss: 0.043872453651964696\n",
      "Epoch 807/1000 - Loss: 0.043773811469857205\n",
      "Epoch 808/1000 - Loss: 0.043675566650476956\n",
      "Epoch 809/1000 - Loss: 0.043577716904601424\n",
      "Epoch 810/1000 - Loss: 0.04348025996009351\n",
      "Epoch 811/1000 - Loss: 0.04338319356174594\n",
      "Epoch 812/1000 - Loss: 0.043286515471125366\n",
      "Epoch 813/1000 - Loss: 0.043190223466421346\n",
      "Epoch 814/1000 - Loss: 0.043094315342294055\n",
      "Epoch 815/1000 - Loss: 0.04299878890972523\n",
      "Epoch 816/1000 - Loss: 0.0429036419958717\n",
      "Epoch 817/1000 - Loss: 0.042808872443915486\n",
      "Epoch 818/1000 - Loss: 0.04271447811292302\n",
      "Epoch 819/1000 - Loss: 0.042620456877699386\n",
      "Epoch 820/1000 - Loss: 0.04252680662864823\n",
      "Epoch 821/1000 - Loss: 0.042433525271630024\n",
      "Epoch 822/1000 - Loss: 0.042340610727824135\n",
      "Epoch 823/1000 - Loss: 0.042248060933592985\n",
      "Epoch 824/1000 - Loss: 0.04215587384034499\n",
      "Epoch 825/1000 - Loss: 0.042064047414398\n",
      "Epoch 826/1000 - Loss: 0.04197257963685054\n",
      "Epoch 827/1000 - Loss: 0.041881468503448205\n",
      "Epoch 828/1000 - Loss: 0.04179071202445352\n",
      "Epoch 829/1000 - Loss: 0.04170030822451716\n",
      "Epoch 830/1000 - Loss: 0.041610255142550345\n",
      "Epoch 831/1000 - Loss: 0.04152055083160072\n",
      "Epoch 832/1000 - Loss: 0.04143119335872406\n",
      "Epoch 833/1000 - Loss: 0.041342180804863085\n",
      "Epoch 834/1000 - Loss: 0.041253511264726206\n",
      "Epoch 835/1000 - Loss: 0.041165182846663526\n",
      "Epoch 836/1000 - Loss: 0.04107719367255025\n",
      "Epoch 837/1000 - Loss: 0.040989541877665986\n",
      "Epoch 838/1000 - Loss: 0.040902225610578644\n",
      "Epoch 839/1000 - Loss: 0.04081524303302923\n",
      "Epoch 840/1000 - Loss: 0.04072859231981465\n",
      "Epoch 841/1000 - Loss: 0.04064227165867558\n",
      "Epoch 842/1000 - Loss: 0.04055627925018561\n",
      "Epoch 843/1000 - Loss: 0.0404706133076361\n",
      "Epoch 844/1000 - Loss: 0.04038527205692914\n",
      "Epoch 845/1000 - Loss: 0.04030025373646761\n",
      "Epoch 846/1000 - Loss: 0.04021555659704798\n",
      "Epoch 847/1000 - Loss: 0.040131178901750414\n",
      "Epoch 848/1000 - Loss: 0.040047118925837295\n",
      "Epoch 849/1000 - Loss: 0.03996337495664625\n",
      "Epoch 850/1000 - Loss: 0.0398799452934855\n",
      "Epoch 851/1000 - Loss: 0.03979682824753509\n",
      "Epoch 852/1000 - Loss: 0.03971402214174033\n",
      "Epoch 853/1000 - Loss: 0.03963152531071662\n",
      "Epoch 854/1000 - Loss: 0.039549336100644025\n",
      "Epoch 855/1000 - Loss: 0.039467452869174656\n",
      "Epoch 856/1000 - Loss: 0.039385873985332845\n",
      "Epoch 857/1000 - Loss: 0.03930459782941587\n",
      "Epoch 858/1000 - Loss: 0.039223622792903076\n",
      "Epoch 859/1000 - Loss: 0.039142947278357806\n",
      "Epoch 860/1000 - Loss: 0.03906256969933633\n",
      "Epoch 861/1000 - Loss: 0.038982488480293495\n",
      "Epoch 862/1000 - Loss: 0.038902702056490934\n",
      "Epoch 863/1000 - Loss: 0.03882320887390646\n",
      "Epoch 864/1000 - Loss: 0.03874400738914478\n",
      "Epoch 865/1000 - Loss: 0.038665096069348065\n",
      "Epoch 866/1000 - Loss: 0.038586473392107204\n",
      "Epoch 867/1000 - Loss: 0.03850813784537457\n",
      "Epoch 868/1000 - Loss: 0.03843008792737858\n",
      "Epoch 869/1000 - Loss: 0.03835232214653658\n",
      "Epoch 870/1000 - Loss: 0.038274839021371576\n",
      "Epoch 871/1000 - Loss: 0.038197637080427796\n",
      "Epoch 872/1000 - Loss: 0.038120714862186114\n",
      "Epoch 873/1000 - Loss: 0.03804407091498473\n",
      "Epoch 874/1000 - Loss: 0.03796770379693355\n",
      "Epoch 875/1000 - Loss: 0.03789161207583905\n",
      "Epoch 876/1000 - Loss: 0.037815794329119086\n",
      "Epoch 877/1000 - Loss: 0.03774024914372591\n",
      "Epoch 878/1000 - Loss: 0.037664975116069435\n",
      "Epoch 879/1000 - Loss: 0.03758997085193611\n",
      "Epoch 880/1000 - Loss: 0.03751523496641555\n",
      "Epoch 881/1000 - Loss: 0.037440766083821755\n",
      "Epoch 882/1000 - Loss: 0.03736656283762033\n",
      "Epoch 883/1000 - Loss: 0.03729262387034994\n",
      "Epoch 884/1000 - Loss: 0.03721894783355377\n",
      "Epoch 885/1000 - Loss: 0.03714553338770066\n",
      "Epoch 886/1000 - Loss: 0.03707237920211802\n",
      "Epoch 887/1000 - Loss: 0.03699948395491573\n",
      "Epoch 888/1000 - Loss: 0.03692684633291608\n",
      "Epoch 889/1000 - Loss: 0.03685446503158572\n",
      "Epoch 890/1000 - Loss: 0.03678233875496266\n",
      "Epoch 891/1000 - Loss: 0.03671046621558902\n",
      "Epoch 892/1000 - Loss: 0.036638846134442474\n",
      "Epoch 893/1000 - Loss: 0.036567477240867396\n",
      "Epoch 894/1000 - Loss: 0.03649635827250923\n",
      "Epoch 895/1000 - Loss: 0.03642548797524674\n",
      "Epoch 896/1000 - Loss: 0.03635486510312754\n",
      "Epoch 897/1000 - Loss: 0.03628448841829948\n",
      "Epoch 898/1000 - Loss: 0.036214356690951216\n",
      "Epoch 899/1000 - Loss: 0.03614446869924469\n",
      "Epoch 900/1000 - Loss: 0.036074823229251034\n",
      "Epoch 901/1000 - Loss: 0.03600541907489037\n",
      "Epoch 902/1000 - Loss: 0.035936255037868486\n",
      "Epoch 903/1000 - Loss: 0.03586732992761364\n",
      "Epoch 904/1000 - Loss: 0.03579864256121863\n",
      "Epoch 905/1000 - Loss: 0.03573019176337715\n",
      "Epoch 906/1000 - Loss: 0.03566197636632619\n",
      "Epoch 907/1000 - Loss: 0.03559399520978482\n",
      "Epoch 908/1000 - Loss: 0.03552624714089676\n",
      "Epoch 909/1000 - Loss: 0.035458731014171185\n",
      "Epoch 910/1000 - Loss: 0.035391445691425695\n",
      "Epoch 911/1000 - Loss: 0.03532439004172852\n",
      "Epoch 912/1000 - Loss: 0.03525756294134245\n",
      "Epoch 913/1000 - Loss: 0.03519096327366743\n",
      "Epoch 914/1000 - Loss: 0.03512458992918675\n",
      "Epoch 915/1000 - Loss: 0.03505844180541084\n",
      "Epoch 916/1000 - Loss: 0.03499251780682221\n",
      "Epoch 917/1000 - Loss: 0.03492681684482277\n",
      "Epoch 918/1000 - Loss: 0.03486133783767805\n",
      "Epoch 919/1000 - Loss: 0.034796079710466804\n",
      "Epoch 920/1000 - Loss: 0.03473104139502622\n",
      "Epoch 921/1000 - Loss: 0.034666221829900756\n",
      "Epoch 922/1000 - Loss: 0.03460161996028964\n",
      "Epoch 923/1000 - Loss: 0.03453723473799718\n",
      "Epoch 924/1000 - Loss: 0.03447306512138061\n",
      "Epoch 925/1000 - Loss: 0.03440911007529924\n",
      "Epoch 926/1000 - Loss: 0.03434536857106756\n",
      "Epoch 927/1000 - Loss: 0.03428183958640267\n",
      "Epoch 928/1000 - Loss: 0.03421852210537703\n",
      "Epoch 929/1000 - Loss: 0.03415541511836912\n",
      "Epoch 930/1000 - Loss: 0.034092517622016254\n",
      "Epoch 931/1000 - Loss: 0.0340298286191667\n",
      "Epoch 932/1000 - Loss: 0.03396734711883141\n",
      "Epoch 933/1000 - Loss: 0.03390507213614053\n",
      "Epoch 934/1000 - Loss: 0.0338430026922915\n",
      "Epoch 935/1000 - Loss: 0.03378113781450817\n",
      "Epoch 936/1000 - Loss: 0.0337194765359938\n",
      "Epoch 937/1000 - Loss: 0.033658017895884976\n",
      "Epoch 938/1000 - Loss: 0.03359676093920752\n",
      "Epoch 939/1000 - Loss: 0.033535704716831385\n",
      "Epoch 940/1000 - Loss: 0.033474848285430084\n",
      "Epoch 941/1000 - Loss: 0.03341419070743167\n",
      "Epoch 942/1000 - Loss: 0.033353731050980495\n",
      "Epoch 943/1000 - Loss: 0.03329346838989195\n",
      "Epoch 944/1000 - Loss: 0.03323340180361023\n",
      "Epoch 945/1000 - Loss: 0.03317353037716739\n",
      "Epoch 946/1000 - Loss: 0.033113853201141116\n",
      "Epoch 947/1000 - Loss: 0.0330543693716129\n",
      "Epoch 948/1000 - Loss: 0.03299507799012716\n",
      "Epoch 949/1000 - Loss: 0.032935978163654046\n",
      "Epoch 950/1000 - Loss: 0.03287706900454258\n",
      "Epoch 951/1000 - Loss: 0.032818349630487006\n",
      "Epoch 952/1000 - Loss: 0.032759819164484845\n",
      "Epoch 953/1000 - Loss: 0.0327014767347967\n",
      "Epoch 954/1000 - Loss: 0.032643321474910385\n",
      "Epoch 955/1000 - Loss: 0.03258535252349862\n",
      "Epoch 956/1000 - Loss: 0.03252756902438482\n",
      "Epoch 957/1000 - Loss: 0.032469970126502615\n",
      "Epoch 958/1000 - Loss: 0.03241255498385942\n",
      "Epoch 959/1000 - Loss: 0.03235532275549893\n",
      "Epoch 960/1000 - Loss: 0.03229827260546529\n",
      "Epoch 961/1000 - Loss: 0.03224140370276484\n",
      "Epoch 962/1000 - Loss: 0.03218471522133148\n",
      "Epoch 963/1000 - Loss: 0.03212820633998982\n",
      "Epoch 964/1000 - Loss: 0.03207187624242059\n",
      "Epoch 965/1000 - Loss: 0.032015724117124285\n",
      "Epoch 966/1000 - Loss: 0.03195974915738767\n",
      "Epoch 967/1000 - Loss: 0.03190395056124638\n",
      "Epoch 968/1000 - Loss: 0.0318483275314548\n",
      "Epoch 969/1000 - Loss: 0.03179287927544722\n",
      "Epoch 970/1000 - Loss: 0.03173760500530788\n",
      "Epoch 971/1000 - Loss: 0.031682503937736126\n",
      "Epoch 972/1000 - Loss: 0.03162757529401241\n",
      "Epoch 973/1000 - Loss: 0.03157281829996699\n",
      "Epoch 974/1000 - Loss: 0.031518232185945204\n",
      "Epoch 975/1000 - Loss: 0.031463816186777224\n",
      "Epoch 976/1000 - Loss: 0.03140956954174481\n",
      "Epoch 977/1000 - Loss: 0.03135549149455028\n",
      "Epoch 978/1000 - Loss: 0.03130158129328314\n",
      "Epoch 979/1000 - Loss: 0.0312478381903915\n",
      "Epoch 980/1000 - Loss: 0.03119426144264864\n",
      "Epoch 981/1000 - Loss: 0.031140850311124468\n",
      "Epoch 982/1000 - Loss: 0.031087604061152775\n",
      "Epoch 983/1000 - Loss: 0.031034521962302976\n",
      "Epoch 984/1000 - Loss: 0.03098160328834815\n",
      "Epoch 985/1000 - Loss: 0.030928847317236937\n",
      "Epoch 986/1000 - Loss: 0.030876253331064196\n",
      "Epoch 987/1000 - Loss: 0.030823820616040234\n",
      "Epoch 988/1000 - Loss: 0.030771548462462953\n",
      "Epoch 989/1000 - Loss: 0.030719436164689223\n",
      "Epoch 990/1000 - Loss: 0.0306674830211053\n",
      "Epoch 991/1000 - Loss: 0.030615688334099195\n",
      "Epoch 992/1000 - Loss: 0.03056405141003362\n",
      "Epoch 993/1000 - Loss: 0.03051257155921626\n",
      "Epoch 994/1000 - Loss: 0.03046124809587404\n",
      "Epoch 995/1000 - Loss: 0.030410080338123813\n",
      "Epoch 996/1000 - Loss: 0.030359067607948004\n",
      "Epoch 997/1000 - Loss: 0.030308209231164045\n",
      "Epoch 998/1000 - Loss: 0.030257504537400847\n",
      "Epoch 999/1000 - Loss: 0.030206952860071554\n",
      "Epoch 1000/1000 - Loss: 0.030156553536346296\n"
     ]
    }
   ],
   "source": [
    "# Sample matrices\n",
    "\n",
    "# Sample matrices\n",
    "input_matrix = np.array([\n",
    "    [1, 0, 0, 0], \n",
    "    [0, 1, 0, 0]\n",
    "])\n",
    "output_matrix = np.array([\n",
    "    [0, 1, 0, 0],\n",
    "    [1, 0, 0, 0]\n",
    "])\n",
    "\n",
    "\n",
    "model = Word2VecSkipgram(embedding_dim=3, learning_rate=0.01)\n",
    "model.train(input_matrix, output_matrix, epochs=1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 last method (used matrix and find loss and update )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_matrix = tf.convert_to_tensor(tf_idf_matrix.toarray(), dtype=tf.float32)\n",
    "target_matrix = tf.convert_to_tensor(tf_idf_matrix.toarray(), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_matrix shape: (29442, 12070)\n",
      "target_matrix shape: (29442, 12070)\n",
      "context_matrix shape: (29442, 12070)\n"
     ]
    }
   ],
   "source": [
    "print(f\"input_matrix shape: {input_matrix.shape}\")\n",
    "print(f\"target_matrix shape: {target_matrix.shape}\")\n",
    "    \n",
    "print(f\"context_matrix shape: {context_matrix.shape}\")\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import spmatrix\n",
    "import tensorflow as tf\n",
    "\n",
    "def softmax(X):\n",
    "    X_exp = tf.exp(X)\n",
    "    partition = tf.reduce_sum(X_exp, 1, keepdims=True)\n",
    "    return X_exp / partition \n",
    "\n",
    "# Assuming you have 29442 unique sentences and 12070 unique words\n",
    "unique_sentences = 29442\n",
    "unique_words = 12070\n",
    "\n",
    "# Random initialization of matrices\n",
    "input_matrix = np.random.rand(unique_sentences, unique_words)\n",
    "target_matrix = np.random.rand(unique_sentences, unique_words)\n",
    "context_matrix = np.random.rand(unique_sentences, unique_words)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "convergence_threshold = 1e-5\n",
    "\n",
    "# Initialize previous loss to infinity for convergence checking\n",
    "previous_loss = float('inf')\n",
    "\n",
    "# Start the training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Predictions and hidden layer computation for the entire batch\n",
    "    hidden_matrix = np.matmul(input_matrix, np.transpose(target_matrix))\n",
    "    output_matrix_predicted = np.matmul(hidden_matrix, context_matrix)\n",
    "    predicted_probabilities = softmax(output_matrix_predicted)  # Replaced bidirectional_softmax with softmax\n",
    "    \n",
    "    # Convert to dense matrices if they are sparse\n",
    "    if isinstance(output_matrix, spmatrix):\n",
    "        output_matrix_dense = output_matrix.toarray()\n",
    "    else:\n",
    "        output_matrix_dense = output_matrix\n",
    "    \n",
    "    # Transpose the output matrix as per your description\n",
    "    loss_matrix = output_matrix_dense.T\n",
    "\n",
    "    \n",
    "    # Compute the loss for all sentences\n",
    "    log_probs = -np.log(predicted_probabilities + 1e-7)\n",
    "    loss_per_element = np.array([np.dot(log_probs[i, :], loss_matrix[:, i]) for i in range(predicted_probabilities.shape[0])])\n",
    "    \n",
    "    total_loss = np.sum(loss_per_element)\n",
    "    \n",
    "    # Gradient computation\n",
    "    dz = predicted_probabilities - output_matrix\n",
    "    dz_transpose = np.transpose(dz)\n",
    "    delta_matrix = np.matmul(context_matrix, dz_transpose)\n",
    "    d_target_matrix = np.matmul(delta_matrix, input_matrix)\n",
    "    d_context_matrix = np.matmul(np.transpose(hidden_matrix), dz)\n",
    "    \n",
    "    # Update matrices\n",
    "    target_matrix -= learning_rate * d_target_matrix\n",
    "    context_matrix -= learning_rate * d_context_matrix\n",
    "    \n",
    "    # Print the average loss for this epoch\n",
    "    avg_loss = total_loss / (unique_sentences*unique_words) # Assuming 1000 sentences\n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss}\")\n",
    "\n",
    "    # Check for convergence\n",
    "    if abs(previous_loss - avg_loss) < convergence_threshold:\n",
    "        print(\"Convergence reached!\")\n",
    "        break\n",
    "    previous_loss = avg_loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
